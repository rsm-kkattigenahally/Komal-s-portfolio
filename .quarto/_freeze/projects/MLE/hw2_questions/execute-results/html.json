{
  "hash": "f2d79802dcfef97c8c10f0fe7c9a6ca7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Poisson Regression Examples\"\nauthor: \"Komal Nagaraj Kattigenahally\"\ndate: today\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n---\n\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n\nLet's read the data for Blueprinty's\n\n::: {#266680ff .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nblueprint = pd.read_csv('blueprinty.csv')\nblueprint.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1051}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patents</th>\n      <th>region</th>\n      <th>age</th>\n      <th>iscustomer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Midwest</td>\n      <td>32.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>37.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Northwest</td>\n      <td>27.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Northeast</td>\n      <td>24.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>37.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#868c239e .cell execution_count=2}\n``` {.python .cell-code}\nmean_iscust = blueprint[blueprint['iscustomer'] == 1]['patents'].mean()\nmean_isnot_cust = blueprint[blueprint['iscustomer'] == 0]['patents'].mean()\nprint(\"Mean number of patents for Blueprinty's customers: \", round(mean_iscust, 3))\nprint(\"Mean number of patents of non-Blueprinty's customers: \", round(mean_isnot_cust, 2))\n\n# Histogram of number of patents for Blueprinty customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 1]['patents'], bins=20, color='blue', alpha=0.7)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents - Blueprint Customers')\nplt.grid(True)\nplt.show()\n\n# Histogram of number of patents for non-customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 0]['patents'], bins=20, color='orange', alpha=0.7)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title(\"Histogram of Patents - Non-Customers\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean number of patents for Blueprinty's customers:  4.133\nMean number of patents of non-Blueprinty's customers:  3.47\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-3-output-2.png){width=659 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-3-output-3.png){width=668 height=449}\n:::\n:::\n\n\nFrom the above histograms of number of patents by customer status it is observed that, for non customers the highest number of patents are around 2-3 and for Blueprinty's customers it is around 4-5. The mean number of patents for Blueprinty's customers is 4.133 and for non customters it is 3.47.\nA larger proportion of Blueprinty's customers have large number of patents (8-16) as compared to that of non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nLet's vizualize the age distribution of customers vs non-customers\n\n::: {#a36648b4 .cell execution_count=3}\n``` {.python .cell-code}\n# Plot for customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 1]['age'], bins=20, color='purple', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution - Customers')\nplt.grid(True)\nplt.show()\n\n# Plot for non-customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 0]['age'], bins=20, color='skyblue', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution - Non-Customers')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-4-output-1.png){width=659 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-4-output-2.png){width=668 height=449}\n:::\n:::\n\n\nFrom the above age distributions of Blueprinty's current customers it is seen that - \n\n- The distribution is spread out, but heavily skewed toward younger ages (18–30)\n- Due to its concentration in younger age group this suggests Blueprint is more attractive to younger individuals or startups.\n\nThe non-customer age distribution suggests the following - \n\n- It is more concentrated, suggesting a larger and more consistent population.\n- The age range is broader and more evenly distributed. This suggests the non-customer segment includes more mature, possibly more established individuals or companies.\n\nLet's visualize the distribution by region for customers vs non-customers\n\n::: {#c1b70839 .cell execution_count=4}\n``` {.python .cell-code}\n# Visualize regional distribution by customer status\n\n# Histogram for customers\nplt.figure(figsize=(8, 5))\ncustomer_regions = blueprint[blueprint['iscustomer'] == 1]['region']\nplt.hist(customer_regions, bins=20, color='violet', alpha=0.7)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Regional Distribution - Customers')\nplt.grid(True)\nplt.show()\n\n# Histogram for non-customers\nplt.figure(figsize=(8, 5))\nnon_customer_regions = blueprint[blueprint['iscustomer'] == 0]['region']\nplt.hist(non_customer_regions, bins=20, color='lightgreen', alpha=0.7)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Regional Distribution - Non-Customers')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-5-output-1.png){width=668 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-5-output-2.png){width=668 height=449}\n:::\n:::\n\n\nFor the Customers of Blueprinty:\n\n- Northeast has the largest number of customers  — over 300 customers.\n- All other regions (Southwest, Midwest, South, Northwest) have much smaller numbers — around 30–50 each.\n- This suggests Blueprint is very strong in the Northeast and underrepresented elsewhere.\n\nFor Non-Customers:\n- The distribution is much more balanced distribution across all regions.\n- Northeast, Southwest, Midwest lead, each with ~250–270.\n- Northwest and South are slightly lower, but still significant (~150+).\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nMathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$\n\nThe likelihood function for the entire sample is:\n\n$$\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAnd the log-likelihood function is:\n\n$$\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n$$\n\nNow we will use Python to estimate the log likelihood function. Code below :\n\n::: {#ddded4d3 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom math import factorial\n\n# Poisson likelihood function\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda <= 0:\n        return -np.inf  # avoid log(0)\n    return -len(Y)*lmbda + np.sum(Y)*np.log(lmbda) - np.sum(np.log([factorial(y) for y in Y]))\n```\n:::\n\n\nNext, we will plot the Poisson Log-Likelihood vs Lambda. This plot visualizes how the Poisson log-likelihood function changes as we vary the rate parameter λ (lambda) — which represents the expected count of patents.\nSome points to note in the plot - \n\n- When λ is too low or too high, the log-likelihood drops, meaning those values poorly explain the data.\n\n- The maximum point on the curve gives the λ that best fits the data — this is the MLE, and is often equal to the sample mean in a simple Poisson model.\n\n- The curve helps us visualize parameter uncertainty: a flatter curve means more uncertainty in the estimate; a steeper curve means the estimate is more precise.\n\n::: {#622175ef .cell execution_count=6}\n``` {.python .cell-code}\nY = blueprint['patents']\nlambda_vals = np.linspace(0.1, 10, 100)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Find MLE\nmle_lambda = lambda_vals[np.argmax(loglik_vals)]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, label=\"Log-Likelihood\", color='blue')\nplt.axvline(x=mle_lambda, color='red', linestyle='--', label=f'MLE = {mle_lambda:.2f}')\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-7-output-1.png){width=757 height=468}\n:::\n:::\n\n\nNow, we will estimate the MLE by making use of the first derivative of log-likelhood function. We will do this with Python :\n\n::: {#9e44ab00 .cell execution_count=7}\n``` {.python .cell-code}\ndef poisson_loglikelihood_derivative(lmbda, Y):\n    if lmbda <= 0:\n        return np.nan  # avoid division by zero\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    return -n + (sum_Y / lmbda)\n\n# Plot log-likelihood and its derivative\nlambda_vals = np.linspace(0.1, 10, 100)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\nderivative_vals = [poisson_loglikelihood_derivative(lmbda, Y) for lmbda in lambda_vals]\n\n# Find MLE\nmle_lambda = lambda_vals[np.argmax(loglik_vals)]\n\n\n# Plot derivative\nplt.plot(lambda_vals, derivative_vals, label=\"First Derivative\", color='green')\nplt.axhline(y=0, color='black', linestyle='--')\nplt.axvline(x=mle_lambda, color='red', linestyle='--', label=\"Zero Crossing (MLE)\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Derivative\")\nplt.title(\"First Derivative of Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-8-output-1.png){width=661 height=468}\n:::\n:::\n\n\nFurther, we will estimate the MLE by optimizing the likelihood function with sp.optimize() in Python\n\n::: {#e4f83a8e .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom math import factorial, log\n\n# Sample observed data\nY = blueprint['patents']\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda_array):\n    lmbda = lmbda_array[0]\n    if lmbda <= 0:\n        return np.inf  # avoid invalid values\n    n = len(Y)\n    sum_Y = sum(Y)\n    const_term = sum([log(factorial(y)) for y in Y])  # optional\n    return -(-n * lmbda + sum_Y * log(lmbda) - const_term)\n\n# Initial guess\ninitial_lambda = [1.0]\n\n# Minimize\nresult = minimize(neg_log_likelihood, initial_lambda, method='L-BFGS-B', bounds=[(0.0001, None)])\n\n# Extract MLE\nlambda_mle = result.x[0]\nprint(f\"MLE for λ using minimize(): {lambda_mle:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLE for λ using minimize(): 3.6847\n```\n:::\n:::\n\n\nThree approaches are used to find the MLE value. the results from all three approaches are as follows :\n\n1. Grid Search and Plotting:\nThe log-likelihood is computed over a grid of λ values and plotted them. The maximum occurred at approximatel 3.68.\n\n2. First Derivative :\nThe first derivate is taken and plotted. The derivative crossed zero at the approximately same λ, confirming the MLE analytically.\n\nNumerical Optimization:\nUsing scipy.optimize.minimize(), we minimized the negative log-likelihood and MLE obtained is 3.6847\n\nThis matched our previous results closely, validating both the numerical and analytical solutions.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n::: {#48c555e2 .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\n\n# Poisson log-likelihood for regression model\ndef poisson_log_likelihood(beta, X, Y):\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n    beta = np.asarray(beta)\n\n    # Linear predictor: Xβ\n    eta = X @ beta\n\n    # Inverse link function: λ = exp(η)\n    lam = np.exp(eta)\n\n    # Log-likelihood\n    loglik = np.sum(Y * np.log(lam) - lam - np.log([np.math.factorial(int(y)) for y in Y]))\n\n    return -loglik  # negative for use with minimize()\n```\n:::\n\n\nNow we use Python's sp.optimze() to find the MLE vector and the Hessian of the Poisson model with covariates. \n\n::: {#6ea917c1 .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom scipy.special import gammaln\n\n# ---- Feature Engineering ----\nblueprint['age_sq'] = blueprint['age'] ** 2\nregion_dummies = pd.get_dummies(blueprint['region'], drop_first=True)\n\n# ---- Design Matrix X ----\nX = pd.concat([\n    pd.Series(1, index=blueprint.index, name='Intercept'),\n    blueprint[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\nX_matrix =  X.astype(float).values\n\n# ---- Outcome Variable Y ----\nY = blueprint['patents'].values\n\n# ---- Poisson Log-Likelihood Function ----\ndef poisson_log_likelihood(beta, X, Y):\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # safe range for exp\n    lam = np.exp(Xb)\n    loglik = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n    return -loglik  # negative for minimization\n\n# ---- Estimation ----\nbeta_init = np.zeros(X_matrix.shape[1])\nresult = minimize(poisson_log_likelihood, beta_init, args=(X_matrix, Y), method='BFGS')\n\n# ---- Extract Coefficients and Standard Errors ----\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# ---- Results Table ----\nresults_df = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': std_errors\n}, index=X.columns)\n\nprint(results_df.round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Coefficient  Std. Error\nIntercept       -0.5100      0.1931\nage              0.1487      0.0145\nage_sq          -0.0030      0.0003\niscustomer       0.2076      0.0329\nNortheast        0.0292      0.0468\nNorthwest       -0.0176      0.0572\nSouth            0.0566      0.0562\nSouthwest        0.0506      0.0496\n```\n:::\n:::\n\n\nChecking for similar results using Python's sm.GLM() below:\n\n::: {#60f32906 .cell execution_count=11}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\n# Make sure X and Y are both purely numeric\nX_numeric = X.astype(float)\nY_numeric = pd.Series(Y).astype(float)\n\n# Fit GLM model\nmodel = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\nresults = model.fit()\n\n# Print the results\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:32:46   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n```\n:::\n:::\n\n\n**Interpretation** \n\nWe estimated a Poisson regression model to examine the relationship between firm characteristics and the number of patents awarded. The following interpretations can be made based on the results obatined -\n\n- Age: The coefficient for age is positive and statistically significant (p < 0.001), while the coefficient for age_sq is negative and also highly significant. This suggests a nonlinear relationship between firm age and patenting activity. Specifically, patenting increases with age up to a point, then begins to decline — forming an inverted-U relationship\n\n- Customer Status: The variable iscustomer has a positive and significant coefficient (p < 0.001), indicating that firms who are Blueprinty customers file approximately 23% more patents than non-customers, all else equal. This highlights a potential link between Blueprint's services and increased innovation outcomes.\n\n- Region: Coefficients for regional indicators (Northeast, Northwest, South, Southwest) are not statistically significant at the 5% level. This implies that, after controlling for age and customer status, regional differences in patenting are minimal or not detectable in this sample. \n\n- The pseudo R-squared is 0.136, indicating a modest but meaningful improvement over a null model.\n\nNow we estimate the number of patents for customers vs non-customers using the following mthods : We create X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and the fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n::: {#d99292d8 .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\n\n# Create two counterfactual datasets:\n# X_0: everyone is NOT a customer (iscustomer = 0)\n# X_1: everyone IS a customer (iscustomer = 1)\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\n# Convert to matrix form\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted number of patents using fitted model\nXb_0 = np.clip(X_0_matrix @ beta_hat, -20, 20)\nXb_1 = np.clip(X_1_matrix @ beta_hat, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)\ny_pred_1 = np.exp(Xb_1)\n\n# Difference in predicted patent counts\ndelta = y_pred_1 - y_pred_0\naverage_diff = np.mean(delta)\n\nprint(\"Average difference in number of patents for customers vs non customers:\" , round(average_diff,3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage difference in number of patents for customers vs non customers: 0.793\n```\n:::\n:::\n\n\nThe above results imply that Firms using Blueprinty's software are predicted to produce, on average, 0.79 more patents than they would have without it - all else held constant.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n::: {#5b50403d .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Load the Airbnb data (change path if needed)\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# --- Clean and prepare data ---\nairbnb_clean = airbnb[[\n    'room_type', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]].copy()\n\n# Convert price to float\nairbnb_clean['price'] = (\n    airbnb_clean['price'].replace('[\\$,]', '', regex=True).astype(float)\n)\n\n# Drop rows with missing values\nairbnb_clean.dropna(inplace=True)\n\n# Convert categorical variables to dummy variables\nairbnb_encoded = pd.get_dummies(\n    airbnb_clean,\n    columns=['room_type', 'instant_bookable'],\n    drop_first=True  # avoids dummy variable trap\n)\n\n# --- Set up model inputs ---\nX = sm.add_constant(X)\nX = X.astype(float)\nY = Y.astype(float) \n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\nresults = poisson_model.fit()\n\n# --- Print model results ---\nsummary_df = pd.DataFrame({\n    'Coefficient': results.params,\n    'Std. Error': results.bse,\n    'z-value': results.tvalues,\n    'p-value': results.pvalues\n}).round(4)\n\nprint(summary_df)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Coefficient  Std. Error  z-value  p-value\nIntercept       -0.5089      0.1832  -2.7783   0.0055\nage              0.1486      0.0139  10.7162   0.0000\nage_sq          -0.0030      0.0003 -11.5132   0.0000\niscustomer       0.2076      0.0309   6.7192   0.0000\nNortheast        0.0292      0.0436   0.6686   0.5037\nNorthwest       -0.0176      0.0538  -0.3268   0.7438\nSouth            0.0566      0.0527   1.0740   0.2828\nSouthwest        0.0506      0.0472   1.0716   0.2839\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:17: SyntaxWarning:\n\ninvalid escape sequence '\\$'\n\n<>:17: SyntaxWarning:\n\ninvalid escape sequence '\\$'\n\nC:\\Users\\komal\\AppData\\Local\\Temp\\ipykernel_31016\\3702920505.py:17: SyntaxWarning:\n\ninvalid escape sequence '\\$'\n\n```\n:::\n:::\n\n\nEstimating the same model with glm below :\n\n::: {#a5fa5e17 .cell execution_count=14}\n``` {.python .cell-code}\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\nresults = poisson_model.fit()\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:32:46   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n```\n:::\n:::\n\n\nWe estimated a Poisson regression model to understand which listing characteristics predict the number of Airbnb reviews, used here as a proxy for bookings. Several variables emerged as significant predictors:\n\n- Instant bookable status had the strongest effect: listings that allow instant booking received ~40% (exp(0.334))  more reviews, highlighting the importance of booking convenience.\n\n- Cleanliness score was another strong predictor: a 1-point increase in cleanliness rating corresponded to a ~12% (exp(0.113)) increase in reviews.\n\n- Bedrooms were positively associated with reviews — each additional bedroom was linked to an ~8% increase in expected bookings.\n\n- In contrast, shared rooms received ~22% fewer reviews than entire homes/apartments, and private rooms had a slight negative effect as well.\n\n- Bathrooms, review_scores_location, and review_scores_value showed negative associations with review count, suggesting that certain amenities or subjective perceptions may not directly translate to more bookings.\n\n- These results emphasize the importance of user experience factors (cleanliness, ease of booking) and listing type in attracting more guests.\n\n- Pseudo R squared = 0.5649, or ~56% This means the model explains about 56% of the variation in the number of reviews relative to a null model\n\n",
    "supporting": [
      "hw2_questions_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}