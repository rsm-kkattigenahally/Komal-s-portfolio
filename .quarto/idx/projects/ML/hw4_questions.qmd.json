{"title":"Latent class MNL and KNN","markdown":{"yaml":{"title":"Latent class MNL and KNN","author":"Komal Kattigenahally","date":"today","jupyter":"python3"},"headingText":"Latent-Class MNL","containsRefs":false,"markdown":"\n\n\n\n\nIn this project, we will create a latent class MNL for a Yogurt purchase data. \nThe Latent Class Multinomial Logit (LC-MNL) model is an extension of the standard Multinomial Logit (MNL) model that captures unobserved heterogeneity in individual decision-making. While the MNL model assumes all individuals share the same preferences, the LC-MNL model allows for the population to consist of several latent (hidden) segments or classes, each with distinct choice behaviors.\n\n### Utility Specification\n\nLet the utility that individual $n$ obtains from choosing alternative $j$ in class $s$ be:\n\n$$\nU_{nj}^{(s)} = X_{nj}'\\beta_s + \\varepsilon_{nj}\n$$\n\nWhere:\n\n- $X_{nj}$ is the vector of observed attributes of the alternative  \n- $\\beta_s$ is the class-specific coefficient vector  \n- $\\varepsilon_{nj}$ is a random error term  \n\nThe probability that an individual belongs to class $s$ is denoted $\\pi_s$, such that:\n\n$$\n\\sum_{s=1}^{S} \\pi_s = 1\n$$\n\nThe choice probability for individual $n$ choosing alternative $j$, marginalizing over classes, is:\n\n$$\nP_{nj} = \\sum_{s=1}^{S} \\pi_s \\cdot \\frac{\\exp(X_{nj}'\\beta_s)}{\\sum_{k=1}^{J} \\exp(X_{nk}'\\beta_s)}\n$$\n\n\nIn the standard Multinomial Logit (MNL) model, Alternative-Specific Constants (ASCs) are included to account for the inherent preference for each product that is not explained by observable attributes such as price or featured status. These constants capture baseline utility differences between alternatives.\n\nMathematically, the utility of individual $n$ choosing alternative $j$ can be written as:\n\n$$\nU_{nj} = ASC_j + \\beta_1 \\cdot \\text{price}_{nj} + \\beta_2 \\cdot \\text{featured}_{nj} + \\varepsilon_{nj}\n$$\n\n\nWhere $ASC_j$ is the alternative-specific constant for product $j$. One of the ASCs (typically for the base category) is omitted to avoid perfect multicollinearity. The remaining ASCs indicate how much more or less preferred each alternative is compared to the base product, holding all else equal.\n\n#### Yogurt latent class MNL\nIn the Yogurt dataset that we have, anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were \"featured\" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.\n\nData preview - \n\n\n```{python}\nimport pandas as pd\n\nyogurt_data = pd.read_csv('yogurt_data.csv')\nyogurt_data.head(10)\n\n```\n\nWe will first reshape this dataset from wide to long format\n\n```{python}\n## Reshape dataset \nyogurt_data = pd.wide_to_long(yogurt_data,\n                          stubnames=['y', 'f', 'p'],\n                          i='id',\n                          j='product',\n                          sep='',\n                          suffix='[1-4]').reset_index()\n\n# Rename columns for clarity\nyogurt_data = yogurt_data.rename(columns={\n    'y': 'chosen',\n    'f': 'featured',\n    'p': 'price'\n})\nyogurt_data.head(10)\n```\n\nIn the above reshaped data, each row represents a consumer–product combination with with data about if the product was chosen by the consumer, if the product was featured abd the price per ounce for that product.\n\n\n### Standard MNL on Yogurt data\nHere we will fit a standard MNL model on the reshaped dataset. When fitting the MNL model in Python using libraries like statsmodels, the ASCs are not automatically created from a categorical variable like product. Therefore, we manually create dummy variables for alternatives using one-hot encoding. The process are as follows - \n\n1. One-hot encode the product variable: This converts the product IDs into binary variables. We drop one column (e.g., for product 1) to act as the reference category.\n\nThese dummy variables become our ASCs. They allow the model to capture each product's inherent preference or appeal that isn't explained by price or promotion. Without ASCs, the model would wrongly assume all products are equally preferred when price and promotion are the same.\n\n2. Merge dummies into the dataset: After encoding, we merge these dummy columns with the dataset so that they can be used as predictors in the model.\n\nBelow we see the code to carry out the above steps.\n\n```{python}\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(drop='first', sparse_output=False)\nproduct_dummies = encoder.fit_transform(yogurt_data[['product']])\n\n# Create DataFrame with appropriate column names\nproduct_dummies_df = pd.DataFrame(product_dummies, columns=encoder.get_feature_names_out())\n\n# Merge dummies with main data\nyogurt_data = pd.concat([yogurt_data.reset_index(drop=True), product_dummies_df.reset_index(drop=True)], axis=1)\nproduct_dummies_df\n\n# Define independent variables (price, featured, and ASCs)\nX = yogurt_data[['price', 'featured'] + list(product_dummies_df.columns)]\nX = sm.add_constant(X)  # Add intercept\nX\n\n```\nThe independent variables for the model includes:\n\n- price: price per ounce\n\n- featured: whether the product was promoted\n\n- product_2, product_3, product_4: dummy variables representing ASCs for products 2, 3, and 4 (product 1 is the reference)\n\n```{python}\n# Dependent variable: whether the product was chosen (1 if chosen, 0 otherwise)\ny = yogurt_data['chosen']\ny\n```\nThe above data is our dependent variable.\n\n```{python}\n\n# Fit the Multinomial Logit model\nmodel = sm.MNLogit(y, X)\nresult = model.fit()\n\n# Show model summary\nresult.summary()\n```\n\nFrom the above summary we see, that\n\n- Price has a very strong negative effect on the choice. Even a small increase in price substantially  reduced the choice probability.\n\n- Featured promotions positively impact the consumer decision. This effect is small.\n\n- Product 1 is the most preferred product and Product 2 is least preferred.\n\n### Latent class MNL model on Yogurt dataset\nNext, we will fit a Latent-class MNL on the same data.\n\n\n```{python}\nimport numpy as np\nimport biogeme.database as db\nimport biogeme.biogeme as bio\nfrom biogeme.expressions import Beta, log, exp\nfrom biogeme import models\n\ndef lc_mnl(K, df):\n    database = db.Database(\"yogurt\", df)\n    database.variables['Choice'] = df['chosen']\n    av = {1: 1, 2: 1, 3: 1, 4: 1}\n\n    class_utilities = []\n    membership_betas = []\n\n    for k in range(1, K + 1):\n        ASC2 = Beta(f'ASC2_class{k}', 0, None, None, 0)\n        ASC3 = Beta(f'ASC3_class{k}', 0, None, None, 0)\n        ASC4 = Beta(f'ASC4_class{k}', 0, None, None, 0)\n        B_PRICE = Beta(f'B_PRICE_class{k}', 0, None, None, 0)\n        B_FEAT = Beta(f'B_FEAT_class{k}', 0, None, None, 0)\n\n        V = {\n            1: 0,\n            2: ASC2 + B_PRICE * database.variables['price'] + B_FEAT * database.variables['featured'],\n            3: ASC3 + B_PRICE * database.variables['price'] + B_FEAT * database.variables['featured'],\n            4: ASC4 + B_PRICE * database.variables['price'] + B_FEAT * database.variables['featured']\n        }\n\n        logprob = models.loglogit(V, av, database.variables['product'])\n        class_utilities.append(logprob)\n\n        if k < K:\n            pi_k = Beta(f'PI_{k}', 1.0 / K, 0.0001, 0.9999, 0)\n            membership_betas.append(pi_k)\n\n    if K == 2:\n        PI = [membership_betas[0], 1 - membership_betas[0]]\n    else:\n        exp_terms = [exp(beta) for beta in membership_betas]\n        denominator = sum(exp_terms) + 1\n        PI = [term / denominator for term in exp_terms]\n        PI.append(1 - sum(PI))\n\n    loglikelihood = log(sum([PI[k] * exp(class_utilities[k]) for k in range(K)]))\n    biogeme_model = bio.BIOGEME(database, loglikelihood)\n    biogeme_model.modelName = f\"LC_MNL_{K}classes\"\n    results = biogeme_model.estimate()\n\n    return {\n        \"K\": K,\n        \"LogLikelihood\": results.data.logLike,\n        \"NumParams\": results.data.nparam,\n        \"BIC\": -2 * results.data.logLike + results.data.nparam * np.log(df['id'].nunique()),\n        \"Parameters\": results.get_estimated_parameters()\n    }\n\n\n```\nTo determine the optimal number of latent classes, we estimate LC-MNL models for 2, 3, 4, and 5 classes and compare them using the Bayesian Information Criterion (BIC) given by :\n\n$BIC = -2*\\ell_n  + k*log(n)$? \n\n(where $\\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) \n\nBIC is a widely used tool for comparing models in terms of both their fit and parsimony. Unlike the raw log-likelihood, which only measures how well a model explains the data, the BIC includes a penalty for model complexity—specifically, the number of estimated parameters. This ensures that adding more classes (which almost always improves fit) is only favored if the improvement is substantial enough to justify the added complexity.\n\nIn the context of the LC-MNL model, BIC allows us to determine the optimal number of latent classes. Each additional class introduces its own set of parameters (utility coefficients and class probabilities), which can risk overfitting if not justified by significant gains in likelihood.\n\n```{python}\nresults_list = []\nfor K in range(2, 6):\n    print(f\"Estimating model for {K} classes...\")\n    res = lc_mnl(K, yogurt_data)\n    results_list.append(res)\n    #print(f\"Estimated parameters for K = {K}:\")\n    #print(res[\"Parameters\"])\n\nbic_df = pd.DataFrame(results_list).sort_values(by='BIC')\n\n```\n\n```{python}\nbic_df[['K', 'LogLikelihood', 'NumParams', 'BIC']]\n```\nThe model with the lowest BIC is selected as the best-fitting model when balancing accuracy and simplicity. In our results, the 3-class model had the lowest BIC, suggesting that it best explains the data while avoiding unnecessary complexity.\n\n#### Comparison of Aggregate MNL vs. Latent-Class MNL (K = 3)\n\nNow we compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC.\n\n```{python}\nlc_mnl_3class_params = results_list[[res[\"K\"] for res in results_list].index(3)][\"Parameters\"].reset_index()\nlc_mnl_3class_params.columns\nclass3_params = lc_mnl_3class_params.loc[lc_mnl_3class_params['index'].str.contains('_class3')]\nprint(\"Class 3 parameter estimates\")\nprint(class3_params)\n```\n\n\n#### 1. Price Sensitivity\n\n| Model            | Price Coefficient        | Interpretation                                                |\n|------------------|--------------------------|----------------------------------------------------------------|\n| Aggregate MNL    | –31.98 (significant)     | Consumers are price-sensitive overall.                         |\n| LC-MNL Class 1   | –3317.95 (not significant) | Very large, likely unstable estimate.                          |\n| LC-MNL Class 2   | –2799.99 (**significant**) | Very strong price aversion.                                    |\n| LC-MNL Class 3   | +9678.82 (**significant**) | Counterintuitive: price increases utility (possible overfitting or perceived quality). |\n\n---\n\n#### 2. Featured Promotion\n\n| Model            | Featured Coefficient      | Interpretation                                               |\n|------------------|---------------------------|---------------------------------------------------------------|\n| Aggregate MNL    | +0.471 (significant)       | Promotion increases likelihood of choice.                    |\n| LC-MNL Class 1   | +13.75 (significant)       | Strong positive impact of being featured.                    |\n| LC-MNL Class 2   | +19.73 (significant)       | Even stronger promotional effect.                            |\n| LC-MNL Class 3   | –613.93 (significant)      | Strong negative effect — promotions deter choice.            |\n\n---\n\n#### 3. Alternative-Specific Constants (ASCs)\n\n| Product | Aggregate MNL | LC Class 1 | LC Class 2 | LC Class 3 |\n|---------|----------------|------------|------------|------------|\n| ASC2    | –0.5166         | –1.02       | +280.04     | –605.01     |\n| ASC3    | –4.5584         | +222.88     | +0.86       | –609.28     |\n| ASC4    | –1.4179         | –2.92       | +279.78     | –604.22     |\n\nInterpretation:\n- LC-MNL reveals stark contrasts between classes.\n- Class 2 prefers all products highly.\n- Class 3 strongly disfavors all alternatives — unusual, possibly unstable.\n\n---\n\n#### 4. Class Membership Probabilities\n\n| Class    | Share (PI) | Interpretation                                       |\n|----------|------------|------------------------------------------------------|\n| Class 1  | 0.730       | Majority: strong effects for price and promotion.   |\n| Class 2  | 0.999       | Possibly absorbing similar behavior as Class 1.     |\n| Class 3  | ~0          | Tiny segment with extreme (and conflicting) effects.|\n\n---\n\n#### **Conclusion**\n\nThe aggregate MNL provides a stable average picture of consumer behavior: moderate price sensitivity and positive reaction to promotions.\n\nHowever, the 3-class LC-MNL uncovers **rich heterogeneity**:\n- **Class 1** behaves similarly to the aggregate trend.\n- **Class 2** intensifies the promotional impact.\n- **Class 3** behaves unusually, disliking promotions and favoring higher prices — likely capturing edge cases or requiring more robust modeling.\n\nLatent class modeling yields **deeper behavioral insights** that would be hidden in an aggregate approach — but demands careful interpretation, especially for small or extreme segments.\n\n\n## K Nearest Neighbors\n\n### How KNN Works\nGiven a data point $x_{\\text{test}}$ to classify:\n\n1. Compute the **distance** between $x_{\\text{test}}$ and all training points $x_i$.\n2. Identify the $k$ closest training points (called the **k-nearest neighbors**).\n3. Assign the **majority class** label among those $k$ neighbors to $x_{\\text{test}}$.\n\n---\n\n### Euclidean Distance\n\nThe most commonly used distance metric is the **Euclidean distance**, defined for two points \n$x = (x_1, x_2, \\dots, x_d)$ and $z = (z_1, z_2, \\dots, z_d)$ as:\n\n$$\nd(x, z) = \\sqrt{(x_1 - z_1)^2 + (x_2 - z_2)^2 + \\cdots + (x_d - z_d)^2}\n$$\n\nFor our 2D case with features $x_1$ and $x_2$:\n\n$$\nd((x_1, x_2), (z_1, z_2)) = \\sqrt{(x_1 - z_1)^2 + (x_2 - z_2)^2}\n$$\n\n---\n\n### Classification Rule\n\nLet $\\mathcal{N}_k(x)$ denote the set of indices of the $k$ nearest neighbors of point $x$.  \nThen the predicted class $\\hat{y}$ is:\n\n$$\n\\hat{y} = \\arg\\max_{c \\in \\{0,1\\}} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{1}(y_i = c)\n$$\n\nWhere:\n- $\\mathbb{1}(y_i = c)$ is an indicator function (1 if true, 0 if false),\n- This counts how many of the $k$ neighbors belong to class $c$,\n- The class with the highest count becomes the predicted class.\n\n---\n\n### Choosing the Right k\n\n- Small $k$ values (like $k=1$) can be sensitive to noise and may overfit.\n- Larger $k$ values smooth out the decision boundary but may underfit.\n- The optimal $k$ is usually chosen using validation data or cross-validation.\n\n---\n\n\nIn this section, we will explore how the K Nearest Neighbors (KNN) algorithm works using a synthetic dataset. We will implement KNN from scratch, compare it with a built-in classifier, and analyze how the value of k impacts model accuracy.\n\n### Generate Synthetic Data\nThe following code will generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function\n\n```{python}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Generate data\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n\nboundary = np.sin(4 * x1) + x1\ny = (x2 > boundary).astype(int)\n\ndf = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndf\n```\n\nWe can visualize the dataset in 2D, using color to represent the binary class (y). We also overlay the wiggly boundary that separates the two classes.\n\n```{python}\nplt.figure(figsize=(8,6))\nplt.scatter(df['x1'], df['x2'], c=df['y'], cmap='bwr', edgecolor='k')\nx_line = np.linspace(-3, 3, 500)\nboundary_line = np.sin(4 * x_line) + x_line\nplt.plot(x_line, boundary_line, 'k--', label='Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Synthetic Dataset')\nplt.legend()\nplt.show()\n\n```\n\nTo evaluate the generalization performance of our model, we create a new test dataset using a different random seed. This ensures the test data is independent of the training set.\n\n```{python}\nnp.random.seed(19)  # different seed\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test > boundary_test).astype(int)\n\ntest_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\ntest_df\n```\n\n### KNN implementation by hand Vs KNeighborsClassifier\nHere we define a custom KNN classifier using the Euclidean distance between test and training points. For each test instance, the k closest neighbors are selected, and the predicted class is determined by majority vote.\n\n\n```{python}\nimport numpy as np\nfrom scipy.spatial import distance\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef knn_predict(X_train, y_train, X_test, k):\n    y_pred = []\n    for test_point in X_test:\n        dists = [distance.euclidean(test_point, train_point) for train_point in X_train]\n        knn_indices = np.argsort(dists)[:k]\n        knn_labels = y_train[knn_indices]\n        # Use np.bincount safely: labels must be integers starting from 0\n        # If labels are 0 and 1, this is fine\n        majority_vote = np.argmax(np.bincount(knn_labels))\n        y_pred.append(majority_vote)\n    return np.array(y_pred)\n\n# Prepare train and test datasets\nX_train = df[['x1', 'x2']].values\ny_train = df['y'].values.astype(int)  # convert to int\nX_test = test_df[['x1', 'x2']].values\ny_test = test_df['y'].values.astype(int)  # convert to int\n\n# Convert y arrays to numpy integer arrays if needed\ny_train = np.array(y_train).astype(int)\ny_test = np.array(y_test).astype(int)\n\n# Built-in KNN classifier\nclf = KNeighborsClassifier(n_neighbors=5)\nclf.fit(X_train, y_train)\ny_lib_pred = clf.predict(X_test)\n\n# Your manual KNN prediction\ny_hand_pred = knn_predict(X_train, y_train, X_test, k=5)\n\n# Compare predictions\nprint(\"Hand-coded KNN accuracy:\", accuracy_score(y_test, y_hand_pred))\nprint(\"Library KNN accuracy:   \", accuracy_score(y_test, y_lib_pred))\n\n\n```\n\nNow we run our custom KNN function across a range of k values (from 1 to 30) to observe how accuracy varies. This helps us identify the optimal value of k that balances underfitting and overfitting.\n\n```{python}\nfrom sklearn.metrics import accuracy_score\n\nX_train = df[['x1', 'x2']].values\ny_train = df['y'].values\nX_test = test_df[['x1', 'x2']].values\ny_test = test_df['y'].values\n\naccuracies = []\n\nfor k in range(1, 31):\n    y_pred = knn_predict(X_train, y_train, X_test, k)\n    acc = accuracy_score(y_test, y_pred)\n    accuracies.append(acc * 100)\n\n```\n\nWe plot the accuracy of the KNN model as a function of k to visualize the trend and identify the best k value.\n\n```{python}\nplt.figure(figsize=(8,5))\nplt.plot(range(1, 31), accuracies, marker='o')\nplt.xlabel('k')\nplt.ylabel('Accuracy (%)')\nplt.title('KNN Accuracy on Test Data')\nplt.grid(True)\nplt.show()\n\noptimal_k = np.argmax(accuracies) + 1\nprint(f\"Optimal k: {optimal_k} with accuracy: {accuracies[optimal_k-1]:.2f}%\")\n\n```\n\n\n**Inference from the plot the accuracy of the KNN model as a function of k**\n\n- **Highest Accuracy at \\( k = 1 \\)** : The model achieves its **highest accuracy (~92%)** when \\( k = 1 \\).\n\n\n- **Sharp Drop from \\( k = 2 \\) to \\( k = 4 \\)** :  Accuracy declines quickly from ~90% to around 86–87% when \\( k \\) increases from 2 to 4.\n\n\n- **Fluctuations Between \\( k = 5 \\) and \\( k = 15 \\)** :  Accuracy fluctuates without a clear upward or downward trend. This suggests that the model struggles to capture a consistently optimal decision boundary in this range.\n\n\n- **Plateau After \\( k \\geq 15 \\)** : For values of \\( k \\geq 15 \\), accuracy stabilizes around **85–86%**. This trend reflects **underfitting**, where the model becomes too smooth and loses its ability to separate complex patterns in the data.\n\n\n- **Optimal Trade-off**:  While \\( k = 1 \\) yields the best test performance, using a slightly higher \\( k \\) (e.g., \\( k = 3 \\) or \\( k = 5 \\)) may provide a better **bias-variance trade-off**. This reduces the risk of overfitting while maintaining high accuracy.\n\n- **Summary :**\n\n- **Best test accuracy**: \\( k = 1 \\)\n- **Recommended practical range**: \\( k = 3 \\) to \\( k = 5 \\) for more robust results.\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n\n## Latent-Class MNL\n\nIn this project, we will create a latent class MNL for a Yogurt purchase data. \nThe Latent Class Multinomial Logit (LC-MNL) model is an extension of the standard Multinomial Logit (MNL) model that captures unobserved heterogeneity in individual decision-making. While the MNL model assumes all individuals share the same preferences, the LC-MNL model allows for the population to consist of several latent (hidden) segments or classes, each with distinct choice behaviors.\n\n### Utility Specification\n\nLet the utility that individual $n$ obtains from choosing alternative $j$ in class $s$ be:\n\n$$\nU_{nj}^{(s)} = X_{nj}'\\beta_s + \\varepsilon_{nj}\n$$\n\nWhere:\n\n- $X_{nj}$ is the vector of observed attributes of the alternative  \n- $\\beta_s$ is the class-specific coefficient vector  \n- $\\varepsilon_{nj}$ is a random error term  \n\nThe probability that an individual belongs to class $s$ is denoted $\\pi_s$, such that:\n\n$$\n\\sum_{s=1}^{S} \\pi_s = 1\n$$\n\nThe choice probability for individual $n$ choosing alternative $j$, marginalizing over classes, is:\n\n$$\nP_{nj} = \\sum_{s=1}^{S} \\pi_s \\cdot \\frac{\\exp(X_{nj}'\\beta_s)}{\\sum_{k=1}^{J} \\exp(X_{nk}'\\beta_s)}\n$$\n\n\nIn the standard Multinomial Logit (MNL) model, Alternative-Specific Constants (ASCs) are included to account for the inherent preference for each product that is not explained by observable attributes such as price or featured status. These constants capture baseline utility differences between alternatives.\n\nMathematically, the utility of individual $n$ choosing alternative $j$ can be written as:\n\n$$\nU_{nj} = ASC_j + \\beta_1 \\cdot \\text{price}_{nj} + \\beta_2 \\cdot \\text{featured}_{nj} + \\varepsilon_{nj}\n$$\n\n\nWhere $ASC_j$ is the alternative-specific constant for product $j$. One of the ASCs (typically for the base category) is omitted to avoid perfect multicollinearity. The remaining ASCs indicate how much more or less preferred each alternative is compared to the base product, holding all else equal.\n\n#### Yogurt latent class MNL\nIn the Yogurt dataset that we have, anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were \"featured\" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.\n\nData preview - \n\n\n```{python}\nimport pandas as pd\n\nyogurt_data = pd.read_csv('yogurt_data.csv')\nyogurt_data.head(10)\n\n```\n\nWe will first reshape this dataset from wide to long format\n\n```{python}\n## Reshape dataset \nyogurt_data = pd.wide_to_long(yogurt_data,\n                          stubnames=['y', 'f', 'p'],\n                          i='id',\n                          j='product',\n                          sep='',\n                          suffix='[1-4]').reset_index()\n\n# Rename columns for clarity\nyogurt_data = yogurt_data.rename(columns={\n    'y': 'chosen',\n    'f': 'featured',\n    'p': 'price'\n})\nyogurt_data.head(10)\n```\n\nIn the above reshaped data, each row represents a consumer–product combination with with data about if the product was chosen by the consumer, if the product was featured abd the price per ounce for that product.\n\n\n### Standard MNL on Yogurt data\nHere we will fit a standard MNL model on the reshaped dataset. When fitting the MNL model in Python using libraries like statsmodels, the ASCs are not automatically created from a categorical variable like product. Therefore, we manually create dummy variables for alternatives using one-hot encoding. The process are as follows - \n\n1. One-hot encode the product variable: This converts the product IDs into binary variables. We drop one column (e.g., for product 1) to act as the reference category.\n\nThese dummy variables become our ASCs. They allow the model to capture each product's inherent preference or appeal that isn't explained by price or promotion. Without ASCs, the model would wrongly assume all products are equally preferred when price and promotion are the same.\n\n2. Merge dummies into the dataset: After encoding, we merge these dummy columns with the dataset so that they can be used as predictors in the model.\n\nBelow we see the code to carry out the above steps.\n\n```{python}\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(drop='first', sparse_output=False)\nproduct_dummies = encoder.fit_transform(yogurt_data[['product']])\n\n# Create DataFrame with appropriate column names\nproduct_dummies_df = pd.DataFrame(product_dummies, columns=encoder.get_feature_names_out())\n\n# Merge dummies with main data\nyogurt_data = pd.concat([yogurt_data.reset_index(drop=True), product_dummies_df.reset_index(drop=True)], axis=1)\nproduct_dummies_df\n\n# Define independent variables (price, featured, and ASCs)\nX = yogurt_data[['price', 'featured'] + list(product_dummies_df.columns)]\nX = sm.add_constant(X)  # Add intercept\nX\n\n```\nThe independent variables for the model includes:\n\n- price: price per ounce\n\n- featured: whether the product was promoted\n\n- product_2, product_3, product_4: dummy variables representing ASCs for products 2, 3, and 4 (product 1 is the reference)\n\n```{python}\n# Dependent variable: whether the product was chosen (1 if chosen, 0 otherwise)\ny = yogurt_data['chosen']\ny\n```\nThe above data is our dependent variable.\n\n```{python}\n\n# Fit the Multinomial Logit model\nmodel = sm.MNLogit(y, X)\nresult = model.fit()\n\n# Show model summary\nresult.summary()\n```\n\nFrom the above summary we see, that\n\n- Price has a very strong negative effect on the choice. Even a small increase in price substantially  reduced the choice probability.\n\n- Featured promotions positively impact the consumer decision. This effect is small.\n\n- Product 1 is the most preferred product and Product 2 is least preferred.\n\n### Latent class MNL model on Yogurt dataset\nNext, we will fit a Latent-class MNL on the same data.\n\n\n```{python}\nimport numpy as np\nimport biogeme.database as db\nimport biogeme.biogeme as bio\nfrom biogeme.expressions import Beta, log, exp\nfrom biogeme import models\n\ndef lc_mnl(K, df):\n    database = db.Database(\"yogurt\", df)\n    database.variables['Choice'] = df['chosen']\n    av = {1: 1, 2: 1, 3: 1, 4: 1}\n\n    class_utilities = []\n    membership_betas = []\n\n    for k in range(1, K + 1):\n        ASC2 = Beta(f'ASC2_class{k}', 0, None, None, 0)\n        ASC3 = Beta(f'ASC3_class{k}', 0, None, None, 0)\n        ASC4 = Beta(f'ASC4_class{k}', 0, None, None, 0)\n        B_PRICE = Beta(f'B_PRICE_class{k}', 0, None, None, 0)\n        B_FEAT = Beta(f'B_FEAT_class{k}', 0, None, None, 0)\n\n        V = {\n            1: 0,\n            2: ASC2 + B_PRICE * database.variables['price'] + B_FEAT * database.variables['featured'],\n            3: ASC3 + B_PRICE * database.variables['price'] + B_FEAT * database.variables['featured'],\n            4: ASC4 + B_PRICE * database.variables['price'] + B_FEAT * database.variables['featured']\n        }\n\n        logprob = models.loglogit(V, av, database.variables['product'])\n        class_utilities.append(logprob)\n\n        if k < K:\n            pi_k = Beta(f'PI_{k}', 1.0 / K, 0.0001, 0.9999, 0)\n            membership_betas.append(pi_k)\n\n    if K == 2:\n        PI = [membership_betas[0], 1 - membership_betas[0]]\n    else:\n        exp_terms = [exp(beta) for beta in membership_betas]\n        denominator = sum(exp_terms) + 1\n        PI = [term / denominator for term in exp_terms]\n        PI.append(1 - sum(PI))\n\n    loglikelihood = log(sum([PI[k] * exp(class_utilities[k]) for k in range(K)]))\n    biogeme_model = bio.BIOGEME(database, loglikelihood)\n    biogeme_model.modelName = f\"LC_MNL_{K}classes\"\n    results = biogeme_model.estimate()\n\n    return {\n        \"K\": K,\n        \"LogLikelihood\": results.data.logLike,\n        \"NumParams\": results.data.nparam,\n        \"BIC\": -2 * results.data.logLike + results.data.nparam * np.log(df['id'].nunique()),\n        \"Parameters\": results.get_estimated_parameters()\n    }\n\n\n```\nTo determine the optimal number of latent classes, we estimate LC-MNL models for 2, 3, 4, and 5 classes and compare them using the Bayesian Information Criterion (BIC) given by :\n\n$BIC = -2*\\ell_n  + k*log(n)$? \n\n(where $\\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) \n\nBIC is a widely used tool for comparing models in terms of both their fit and parsimony. Unlike the raw log-likelihood, which only measures how well a model explains the data, the BIC includes a penalty for model complexity—specifically, the number of estimated parameters. This ensures that adding more classes (which almost always improves fit) is only favored if the improvement is substantial enough to justify the added complexity.\n\nIn the context of the LC-MNL model, BIC allows us to determine the optimal number of latent classes. Each additional class introduces its own set of parameters (utility coefficients and class probabilities), which can risk overfitting if not justified by significant gains in likelihood.\n\n```{python}\nresults_list = []\nfor K in range(2, 6):\n    print(f\"Estimating model for {K} classes...\")\n    res = lc_mnl(K, yogurt_data)\n    results_list.append(res)\n    #print(f\"Estimated parameters for K = {K}:\")\n    #print(res[\"Parameters\"])\n\nbic_df = pd.DataFrame(results_list).sort_values(by='BIC')\n\n```\n\n```{python}\nbic_df[['K', 'LogLikelihood', 'NumParams', 'BIC']]\n```\nThe model with the lowest BIC is selected as the best-fitting model when balancing accuracy and simplicity. In our results, the 3-class model had the lowest BIC, suggesting that it best explains the data while avoiding unnecessary complexity.\n\n#### Comparison of Aggregate MNL vs. Latent-Class MNL (K = 3)\n\nNow we compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC.\n\n```{python}\nlc_mnl_3class_params = results_list[[res[\"K\"] for res in results_list].index(3)][\"Parameters\"].reset_index()\nlc_mnl_3class_params.columns\nclass3_params = lc_mnl_3class_params.loc[lc_mnl_3class_params['index'].str.contains('_class3')]\nprint(\"Class 3 parameter estimates\")\nprint(class3_params)\n```\n\n\n#### 1. Price Sensitivity\n\n| Model            | Price Coefficient        | Interpretation                                                |\n|------------------|--------------------------|----------------------------------------------------------------|\n| Aggregate MNL    | –31.98 (significant)     | Consumers are price-sensitive overall.                         |\n| LC-MNL Class 1   | –3317.95 (not significant) | Very large, likely unstable estimate.                          |\n| LC-MNL Class 2   | –2799.99 (**significant**) | Very strong price aversion.                                    |\n| LC-MNL Class 3   | +9678.82 (**significant**) | Counterintuitive: price increases utility (possible overfitting or perceived quality). |\n\n---\n\n#### 2. Featured Promotion\n\n| Model            | Featured Coefficient      | Interpretation                                               |\n|------------------|---------------------------|---------------------------------------------------------------|\n| Aggregate MNL    | +0.471 (significant)       | Promotion increases likelihood of choice.                    |\n| LC-MNL Class 1   | +13.75 (significant)       | Strong positive impact of being featured.                    |\n| LC-MNL Class 2   | +19.73 (significant)       | Even stronger promotional effect.                            |\n| LC-MNL Class 3   | –613.93 (significant)      | Strong negative effect — promotions deter choice.            |\n\n---\n\n#### 3. Alternative-Specific Constants (ASCs)\n\n| Product | Aggregate MNL | LC Class 1 | LC Class 2 | LC Class 3 |\n|---------|----------------|------------|------------|------------|\n| ASC2    | –0.5166         | –1.02       | +280.04     | –605.01     |\n| ASC3    | –4.5584         | +222.88     | +0.86       | –609.28     |\n| ASC4    | –1.4179         | –2.92       | +279.78     | –604.22     |\n\nInterpretation:\n- LC-MNL reveals stark contrasts between classes.\n- Class 2 prefers all products highly.\n- Class 3 strongly disfavors all alternatives — unusual, possibly unstable.\n\n---\n\n#### 4. Class Membership Probabilities\n\n| Class    | Share (PI) | Interpretation                                       |\n|----------|------------|------------------------------------------------------|\n| Class 1  | 0.730       | Majority: strong effects for price and promotion.   |\n| Class 2  | 0.999       | Possibly absorbing similar behavior as Class 1.     |\n| Class 3  | ~0          | Tiny segment with extreme (and conflicting) effects.|\n\n---\n\n#### **Conclusion**\n\nThe aggregate MNL provides a stable average picture of consumer behavior: moderate price sensitivity and positive reaction to promotions.\n\nHowever, the 3-class LC-MNL uncovers **rich heterogeneity**:\n- **Class 1** behaves similarly to the aggregate trend.\n- **Class 2** intensifies the promotional impact.\n- **Class 3** behaves unusually, disliking promotions and favoring higher prices — likely capturing edge cases or requiring more robust modeling.\n\nLatent class modeling yields **deeper behavioral insights** that would be hidden in an aggregate approach — but demands careful interpretation, especially for small or extreme segments.\n\n\n## K Nearest Neighbors\n\n### How KNN Works\nGiven a data point $x_{\\text{test}}$ to classify:\n\n1. Compute the **distance** between $x_{\\text{test}}$ and all training points $x_i$.\n2. Identify the $k$ closest training points (called the **k-nearest neighbors**).\n3. Assign the **majority class** label among those $k$ neighbors to $x_{\\text{test}}$.\n\n---\n\n### Euclidean Distance\n\nThe most commonly used distance metric is the **Euclidean distance**, defined for two points \n$x = (x_1, x_2, \\dots, x_d)$ and $z = (z_1, z_2, \\dots, z_d)$ as:\n\n$$\nd(x, z) = \\sqrt{(x_1 - z_1)^2 + (x_2 - z_2)^2 + \\cdots + (x_d - z_d)^2}\n$$\n\nFor our 2D case with features $x_1$ and $x_2$:\n\n$$\nd((x_1, x_2), (z_1, z_2)) = \\sqrt{(x_1 - z_1)^2 + (x_2 - z_2)^2}\n$$\n\n---\n\n### Classification Rule\n\nLet $\\mathcal{N}_k(x)$ denote the set of indices of the $k$ nearest neighbors of point $x$.  \nThen the predicted class $\\hat{y}$ is:\n\n$$\n\\hat{y} = \\arg\\max_{c \\in \\{0,1\\}} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{1}(y_i = c)\n$$\n\nWhere:\n- $\\mathbb{1}(y_i = c)$ is an indicator function (1 if true, 0 if false),\n- This counts how many of the $k$ neighbors belong to class $c$,\n- The class with the highest count becomes the predicted class.\n\n---\n\n### Choosing the Right k\n\n- Small $k$ values (like $k=1$) can be sensitive to noise and may overfit.\n- Larger $k$ values smooth out the decision boundary but may underfit.\n- The optimal $k$ is usually chosen using validation data or cross-validation.\n\n---\n\n\nIn this section, we will explore how the K Nearest Neighbors (KNN) algorithm works using a synthetic dataset. We will implement KNN from scratch, compare it with a built-in classifier, and analyze how the value of k impacts model accuracy.\n\n### Generate Synthetic Data\nThe following code will generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function\n\n```{python}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Generate data\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n\nboundary = np.sin(4 * x1) + x1\ny = (x2 > boundary).astype(int)\n\ndf = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndf\n```\n\nWe can visualize the dataset in 2D, using color to represent the binary class (y). We also overlay the wiggly boundary that separates the two classes.\n\n```{python}\nplt.figure(figsize=(8,6))\nplt.scatter(df['x1'], df['x2'], c=df['y'], cmap='bwr', edgecolor='k')\nx_line = np.linspace(-3, 3, 500)\nboundary_line = np.sin(4 * x_line) + x_line\nplt.plot(x_line, boundary_line, 'k--', label='Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Synthetic Dataset')\nplt.legend()\nplt.show()\n\n```\n\nTo evaluate the generalization performance of our model, we create a new test dataset using a different random seed. This ensures the test data is independent of the training set.\n\n```{python}\nnp.random.seed(19)  # different seed\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test > boundary_test).astype(int)\n\ntest_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\ntest_df\n```\n\n### KNN implementation by hand Vs KNeighborsClassifier\nHere we define a custom KNN classifier using the Euclidean distance between test and training points. For each test instance, the k closest neighbors are selected, and the predicted class is determined by majority vote.\n\n\n```{python}\nimport numpy as np\nfrom scipy.spatial import distance\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef knn_predict(X_train, y_train, X_test, k):\n    y_pred = []\n    for test_point in X_test:\n        dists = [distance.euclidean(test_point, train_point) for train_point in X_train]\n        knn_indices = np.argsort(dists)[:k]\n        knn_labels = y_train[knn_indices]\n        # Use np.bincount safely: labels must be integers starting from 0\n        # If labels are 0 and 1, this is fine\n        majority_vote = np.argmax(np.bincount(knn_labels))\n        y_pred.append(majority_vote)\n    return np.array(y_pred)\n\n# Prepare train and test datasets\nX_train = df[['x1', 'x2']].values\ny_train = df['y'].values.astype(int)  # convert to int\nX_test = test_df[['x1', 'x2']].values\ny_test = test_df['y'].values.astype(int)  # convert to int\n\n# Convert y arrays to numpy integer arrays if needed\ny_train = np.array(y_train).astype(int)\ny_test = np.array(y_test).astype(int)\n\n# Built-in KNN classifier\nclf = KNeighborsClassifier(n_neighbors=5)\nclf.fit(X_train, y_train)\ny_lib_pred = clf.predict(X_test)\n\n# Your manual KNN prediction\ny_hand_pred = knn_predict(X_train, y_train, X_test, k=5)\n\n# Compare predictions\nprint(\"Hand-coded KNN accuracy:\", accuracy_score(y_test, y_hand_pred))\nprint(\"Library KNN accuracy:   \", accuracy_score(y_test, y_lib_pred))\n\n\n```\n\nNow we run our custom KNN function across a range of k values (from 1 to 30) to observe how accuracy varies. This helps us identify the optimal value of k that balances underfitting and overfitting.\n\n```{python}\nfrom sklearn.metrics import accuracy_score\n\nX_train = df[['x1', 'x2']].values\ny_train = df['y'].values\nX_test = test_df[['x1', 'x2']].values\ny_test = test_df['y'].values\n\naccuracies = []\n\nfor k in range(1, 31):\n    y_pred = knn_predict(X_train, y_train, X_test, k)\n    acc = accuracy_score(y_test, y_pred)\n    accuracies.append(acc * 100)\n\n```\n\nWe plot the accuracy of the KNN model as a function of k to visualize the trend and identify the best k value.\n\n```{python}\nplt.figure(figsize=(8,5))\nplt.plot(range(1, 31), accuracies, marker='o')\nplt.xlabel('k')\nplt.ylabel('Accuracy (%)')\nplt.title('KNN Accuracy on Test Data')\nplt.grid(True)\nplt.show()\n\noptimal_k = np.argmax(accuracies) + 1\nprint(f\"Optimal k: {optimal_k} with accuracy: {accuracies[optimal_k-1]:.2f}%\")\n\n```\n\n\n**Inference from the plot the accuracy of the KNN model as a function of k**\n\n- **Highest Accuracy at \\( k = 1 \\)** : The model achieves its **highest accuracy (~92%)** when \\( k = 1 \\).\n\n\n- **Sharp Drop from \\( k = 2 \\) to \\( k = 4 \\)** :  Accuracy declines quickly from ~90% to around 86–87% when \\( k \\) increases from 2 to 4.\n\n\n- **Fluctuations Between \\( k = 5 \\) and \\( k = 15 \\)** :  Accuracy fluctuates without a clear upward or downward trend. This suggests that the model struggles to capture a consistently optimal decision boundary in this range.\n\n\n- **Plateau After \\( k \\geq 15 \\)** : For values of \\( k \\geq 15 \\), accuracy stabilizes around **85–86%**. This trend reflects **underfitting**, where the model becomes too smooth and loses its ability to separate complex patterns in the data.\n\n\n- **Optimal Trade-off**:  While \\( k = 1 \\) yields the best test performance, using a slightly higher \\( k \\) (e.g., \\( k = 3 \\) or \\( k = 5 \\)) may provide a better **bias-variance trade-off**. This reduces the risk of overfitting while maintaining high accuracy.\n\n- **Summary :**\n\n- **Best test accuracy**: \\( k = 1 \\)\n- **Recommended practical range**: \\( k = 3 \\) to \\( k = 5 \\) for more robust results.\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"hw4_questions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"Latent class MNL and KNN","author":"Komal Kattigenahally","date":"today","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}