{"title":"MLE and Conjoint Analysis","markdown":{"yaml":{"title":"MLE and Conjoint Analysis","author":"Komal Nagaraj","date":"today"},"headingText":"1. Likelihood for the Multi-nomial Logit (MNL) Model","containsRefs":false,"markdown":"\n\n\nThis assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data.\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = list(range(8, 33, 4))\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brands for a in ads for p in prices],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Simulate one respondent's data\ndef sim_one(resp_id):\n    tasks = []\n\n    for t in range(1, n_tasks + 1):\n        # Sample 3 alternatives randomly\n        sampled = profiles.sample(n=n_alts, replace=False).copy()\n        sampled[\"resp\"] = resp_id\n        sampled[\"task\"] = t\n\n        # Compute deterministic utility\n        sampled[\"v\"] = sampled[\"brand\"].map(b_util) + \\\n                       sampled[\"ad\"].map(a_util) + \\\n                       sampled[\"price\"].apply(p_util)\n        \n        # Add Gumbel noise (Type I extreme value)\n        sampled[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        # Determine choice\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        tasks.append(sampled[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]])\n\n    return pd.concat(tasks)\n\n# Simulate for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\nconjoint_data\n\n```\n::::\n\n\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n```{python}\n\ndf = pd.get_dummies(conjoint_data, columns=['brand', 'ad'], drop_first=True)\nbool_cols = df.select_dtypes(include='bool').columns\ndf[bool_cols] = df[bool_cols].astype(int)\ndf.head()\n```\n\n\n## 4. Estimation via Maximum Likelihood\n\nNext we define the maximum likelihood function to find the  $\\beta$ values for each parameter.\n\n```{python}\n\nX_cols = ['price', 'brand_N', 'brand_P', 'ad_Yes']\nX = df[X_cols].values\ny = df['choice'].values\n\nn_alts = 3\nn_sets = int(len(df) / n_alts)\n\ndef negative_log_likelihood(beta):\n    ll = 0\n    for i in range(n_sets):\n        start = i * n_alts\n        end = start + n_alts\n        X_i = X[start:end]\n        y_i = y[start:end]\n        utilities = X_i @ beta\n        utilities -= np.max(utilities)\n        exp_utilities = np.exp(utilities)\n        probabilities = exp_utilities / np.sum(exp_utilities)\n        ll += np.sum(y_i * np.log(probabilities + 1e-12))\n    return -ll\n\n```\n\nFurther, we use the scipy.optimize() function to find the MLEs for the 4 parameters as well as  their standard errors (from the Hessian). We also estimate a 95% confidence interval for each parameter estimate.\n\n```{python}\nprint(\"MLE Estimation of beta\")\nfrom scipy.optimize import minimize\nimport scipy.stats as stats\n# Initial guess for parameters (zeros)\ninitial_beta = np.zeros(X.shape[1])\n\n# Estimate MLE using BFGS method\nresult = minimize(negative_log_likelihood, initial_beta, method='BFGS')\n\n# Estimated parameters\nbeta_hat = result.x\n\n# Inverse of Hessian gives variance-covariance matrix\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# 95% Confidence Intervals\nz = stats.norm.ppf(0.975)  # ≈ 1.96\nconf_intervals = [(b - z*se, b + z*se) for b, se in zip(beta_hat, standard_errors)]\nmle_results={}\n# Display results\nfor name, b, se, ci in zip(X_cols, beta_hat, standard_errors, conf_intervals):\n    print(f\"{name}: beta = {b:.4f}, SE = {se:.4f}, 95% CI = ({ci[0]:.4f}, {ci[1]:.4f})\")\n    mle_results[name] = {\n        \"beta\": b,\n        \"se\": se,\n        \"ci\": (ci[0], ci[1])\n    }\n```\n## 5. Estimation via Bayesian Methods\nIn this section we are estimating the posterior distribution of 4 parameters in a MNL model using Bayesian inference. In other words we try to estimate \"Based on the observed choices (data), and what I believed about the parameters before (priors), what should I now believe (posterior)\"\n\nThe Baye's rule is given by - \n\n$$\nP(\\beta \\mid \\text{data}) \\propto {P(\\text{data} \\mid \\beta) \\cdot P(\\beta)}\n$$\n\nwhere $P(\\beta)$ = Prior\n$P(\\text{data} \\mid \\beta)$ = Likelihood\n$P(\\beta \\mid \\text{data})$ = Posterior\n\n\n```{python}\nn_alts = 3\nn_sets = int(len(df) / n_alts)\nn_params = X.shape[1]\n\ndef log_prior(beta):\n    logp_price = -0.5 * (beta[0] / 1)**2\n    logp_binaries = -0.5 * np.sum((beta[1:] / 5)**2)\n    return logp_price + logp_binaries\n\ndef log_posterior(beta):\n    return log_prior(beta) - negative_log_likelihood(beta) \n\n#Metropolis-Hastings MCMC Sampler\nn_samples = 11000\nburn_in = 1000\nsamples = np.zeros((n_samples, n_params))\n\n# Initial guess\ncurrent_beta = np.zeros(n_params)\ncurrent_log_post = log_posterior(current_beta)\n\n# Proposal SDs: binary vars = 0.05, price = 0.005\nproposal_sds = np.array([0.005, 0.05, 0.05, 0.05])\n\nfor t in range(1, n_samples):\n    proposal = current_beta + np.random.normal(0, proposal_sds)\n    proposal_log_post = log_posterior(proposal)\n    accept_prob = min(1, np.exp(proposal_log_post - current_log_post))\n\n    if np.random.rand() < accept_prob:\n        current_beta = proposal\n        current_log_post = proposal_log_post\n\n    samples[t] = current_beta\n\n# Drop burn-in\nposterior_samples = samples[burn_in:]\nbayes_results={}\n# Posterior summaries\nfor i, name in enumerate(X_cols):\n    param_samples = posterior_samples[:, i]\n    mean = np.mean(param_samples)\n    std = np.std(param_samples)\n    ci_low, ci_high = np.percentile(param_samples, [2.5, 97.5])\n    print(f\"{name}: Beta = {mean:.4f}, SD = {std:.4f}, 95% CI = ({ci_low:.4f}, {ci_high:.4f})\")\n    bayes_results[name] = {\n        \"beta\": mean,\n        \"std\": std,\n        \"ci\": (ci_low, ci_high)\n    }\n```\n\n\nBelow is the trace plot of the algorithm, as well as the histogram of the posterior distribution for the price estimate.\n\n```{python}\nprint(posterior_samples)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming price is at index 3\nprice_samples = posterior_samples[:, 0]\n\n# Plot\nplt.figure(figsize=(12, 4))\n\n# Trace Plot\n# Trace Plot\nplt.plot(price_samples, color='blue', linewidth=0.7)\nplt.title(\"Trace Plot: Price Coefficient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"β (price)\")\nplt.show()\n\n# Posterior Histogram\nplt.hist(price_samples, bins=40, color='skyblue', edgecolor='black', density=True)\nplt.title(\"Posterior Distribution: Price Coefficient\")\nplt.xlabel(\"β (price)\")\nplt.ylabel(\"Density\")\nplt.show()\n```\nThe trace plot for the price coefficient shows stable oscillations around a central value, indicating that the Markov Chain Monte Carlo (MCMC) sampler has converged and is mixing well. There is no upward or downward drift, suggesting the chain is sampling effectively from the posterior distribution.\n\nThe corresponding posterior distribution is approximately normal, centered around -0.10. This confirms a strong negative effect of price on product choice — as price increases, the probability of selection decreases. The tight, symmetric shape of the distribution reflects high certainty in this estimate.\n\nThe following table summarizes the estimates from MLE and Bayesian methods.\n\n```{python}\n#| echo: false\nfrom IPython.display import Markdown\n\n# Build table header\ntable_md = \"| Parameter | MLE Estimate (SE) | 95% CI (MLE) | Bayes Mean (SD) | 95% Credible Interval |\\n\"\ntable_md += \"|-----------|-------------------|--------------|------------------|------------------------|\\n\"\n\n# Fill in rows dynamically\nfor param in ['price', 'brand_N', 'brand_P', 'ad_Yes']:\n    mle = mle_results[param]\n    bayes = bayes_results[param]\n    \n    row = (\n        f\"| **{param}** \"\n        f\"| {mle['beta']:.4f} ({mle['se']:.4f}) \"\n        f\"| ({mle['ci'][0]:.4f}, {mle['ci'][1]:.4f}) \"\n        f\"| {bayes['beta']:.4f} ({bayes['std']:.4f}) \"\n        f\"| ({bayes['ci'][0]:.4f}, {bayes['ci'][1]:.4f}) |\\n\"\n    )\n    table_md += row\n\n# Display the Markdown-formatted table\nMarkdown(table_md)\n```\n## 6. Discussion\n\nInterpretation of Parameter Estimates\nIf the data were not simulated, the parameter estimates would reflect real-world consumer preferences inferred from observed choices. In that case:\n\nA finding like $ \\beta_\\text{Netflix} > \\beta_\\text{Prime} $ means that, on average, consumers derive more utility from choosing Netflix compared to Amazon Prime, holding all other attributes constant.\n\nThis could indicate that consumers perceive higher value or satisfaction from Netflix’s offering (e.g., content library, user experience).\n\nIt also implies that, in the utility function a higher coefficient on Netflix leads to a higher probability of being chosen.\n\nRegarding price:\n\nA negative $\\beta_\\text{price}$ is expected and intuitive. It means that, all else equal, an increase in price decreases utility, which lowers the likelihood of the product being chosen.\n\nThis reflects basic economic theory: consumers prefer lower-cost options when utility from other features is equal.\n\n#### Multi-Level (Hierarchical) Model\n\nIn real-world conjoint analysis, consumer preferences are rarely homogeneous. The basic multinomial logit (MNL) model assumes that every individual shares the same set of preference parameters ($\\beta$), which is a strong and often unrealistic assumption.\n\nTo better reflect real-world heterogeneity in preferences, we use a **multi-level** model. Here's how this model can be both simulated and estimated:\n\n**1. Simulating Individual-Level Preferences**\n\nInstead of one global $\\beta$, we assume each respondent $i$ has their own parameter vector ${\\beta}_i$, drawn from a common population distribution:\n\n$$\n{\\beta}_i \\sim \\mathcal{N}({\\mu}, {\\Sigma})\n$$\n\n- ${\\mu}$: population mean of preferences  \n- ${\\Sigma}$: covariance matrix capturing variability and correlations between parameters  \n\nThis framework captures variation in individual tastes.\n\n---\n\n**2. Simulating Choice Data**\n\nUsing each individual's ${\\beta}_i$, simulate their choices for each task using the softmax choice probability:\n\n$$\nP_{ij} = \\frac{\\exp(\\mathbf{X}_{ij}^\\top {\\beta}_i)}{\\sum_k \\exp(\\mathbf{X}_{ik}^\\top {\\beta}_i)}\n$$\n\nWhere:\n- $j$ indexes alternatives\n- $\\mathbf{X}_{ij}$ are the attributes of alternative $j$\n- $P_{ij}$ is the probability that individual $i$ chooses alternative $j$\n\n---\n\n**3. Estimating the Model: Hierarchical Bayes**\n\nTo estimate a hierarchical model, use Hierarchical Bayesian (HB) methods such as:\n\n- Gibbs sampling  \n- Hamiltonian Monte Carlo \n- MCMC within Gibbs (as in traditional HB packages)\n\nThese methods estimate:\n\n- Individual-level coefficients $\\beta_i$\n- Population-level parameters $\\mu$, $\\Sigma$\n\nThis approach gives:\n- Personalized preference estimates\n- More realistic modeling of population behavior\n- Better predictive performance\n\n---\n\n","srcMarkdownNoYaml":"\n\n\nThis assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n\n## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data.\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = list(range(8, 33, 4))\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brands for a in ads for p in prices],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Simulate one respondent's data\ndef sim_one(resp_id):\n    tasks = []\n\n    for t in range(1, n_tasks + 1):\n        # Sample 3 alternatives randomly\n        sampled = profiles.sample(n=n_alts, replace=False).copy()\n        sampled[\"resp\"] = resp_id\n        sampled[\"task\"] = t\n\n        # Compute deterministic utility\n        sampled[\"v\"] = sampled[\"brand\"].map(b_util) + \\\n                       sampled[\"ad\"].map(a_util) + \\\n                       sampled[\"price\"].apply(p_util)\n        \n        # Add Gumbel noise (Type I extreme value)\n        sampled[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        # Determine choice\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        tasks.append(sampled[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]])\n\n    return pd.concat(tasks)\n\n# Simulate for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\nconjoint_data\n\n```\n::::\n\n\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n```{python}\n\ndf = pd.get_dummies(conjoint_data, columns=['brand', 'ad'], drop_first=True)\nbool_cols = df.select_dtypes(include='bool').columns\ndf[bool_cols] = df[bool_cols].astype(int)\ndf.head()\n```\n\n\n## 4. Estimation via Maximum Likelihood\n\nNext we define the maximum likelihood function to find the  $\\beta$ values for each parameter.\n\n```{python}\n\nX_cols = ['price', 'brand_N', 'brand_P', 'ad_Yes']\nX = df[X_cols].values\ny = df['choice'].values\n\nn_alts = 3\nn_sets = int(len(df) / n_alts)\n\ndef negative_log_likelihood(beta):\n    ll = 0\n    for i in range(n_sets):\n        start = i * n_alts\n        end = start + n_alts\n        X_i = X[start:end]\n        y_i = y[start:end]\n        utilities = X_i @ beta\n        utilities -= np.max(utilities)\n        exp_utilities = np.exp(utilities)\n        probabilities = exp_utilities / np.sum(exp_utilities)\n        ll += np.sum(y_i * np.log(probabilities + 1e-12))\n    return -ll\n\n```\n\nFurther, we use the scipy.optimize() function to find the MLEs for the 4 parameters as well as  their standard errors (from the Hessian). We also estimate a 95% confidence interval for each parameter estimate.\n\n```{python}\nprint(\"MLE Estimation of beta\")\nfrom scipy.optimize import minimize\nimport scipy.stats as stats\n# Initial guess for parameters (zeros)\ninitial_beta = np.zeros(X.shape[1])\n\n# Estimate MLE using BFGS method\nresult = minimize(negative_log_likelihood, initial_beta, method='BFGS')\n\n# Estimated parameters\nbeta_hat = result.x\n\n# Inverse of Hessian gives variance-covariance matrix\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# 95% Confidence Intervals\nz = stats.norm.ppf(0.975)  # ≈ 1.96\nconf_intervals = [(b - z*se, b + z*se) for b, se in zip(beta_hat, standard_errors)]\nmle_results={}\n# Display results\nfor name, b, se, ci in zip(X_cols, beta_hat, standard_errors, conf_intervals):\n    print(f\"{name}: beta = {b:.4f}, SE = {se:.4f}, 95% CI = ({ci[0]:.4f}, {ci[1]:.4f})\")\n    mle_results[name] = {\n        \"beta\": b,\n        \"se\": se,\n        \"ci\": (ci[0], ci[1])\n    }\n```\n## 5. Estimation via Bayesian Methods\nIn this section we are estimating the posterior distribution of 4 parameters in a MNL model using Bayesian inference. In other words we try to estimate \"Based on the observed choices (data), and what I believed about the parameters before (priors), what should I now believe (posterior)\"\n\nThe Baye's rule is given by - \n\n$$\nP(\\beta \\mid \\text{data}) \\propto {P(\\text{data} \\mid \\beta) \\cdot P(\\beta)}\n$$\n\nwhere $P(\\beta)$ = Prior\n$P(\\text{data} \\mid \\beta)$ = Likelihood\n$P(\\beta \\mid \\text{data})$ = Posterior\n\n\n```{python}\nn_alts = 3\nn_sets = int(len(df) / n_alts)\nn_params = X.shape[1]\n\ndef log_prior(beta):\n    logp_price = -0.5 * (beta[0] / 1)**2\n    logp_binaries = -0.5 * np.sum((beta[1:] / 5)**2)\n    return logp_price + logp_binaries\n\ndef log_posterior(beta):\n    return log_prior(beta) - negative_log_likelihood(beta) \n\n#Metropolis-Hastings MCMC Sampler\nn_samples = 11000\nburn_in = 1000\nsamples = np.zeros((n_samples, n_params))\n\n# Initial guess\ncurrent_beta = np.zeros(n_params)\ncurrent_log_post = log_posterior(current_beta)\n\n# Proposal SDs: binary vars = 0.05, price = 0.005\nproposal_sds = np.array([0.005, 0.05, 0.05, 0.05])\n\nfor t in range(1, n_samples):\n    proposal = current_beta + np.random.normal(0, proposal_sds)\n    proposal_log_post = log_posterior(proposal)\n    accept_prob = min(1, np.exp(proposal_log_post - current_log_post))\n\n    if np.random.rand() < accept_prob:\n        current_beta = proposal\n        current_log_post = proposal_log_post\n\n    samples[t] = current_beta\n\n# Drop burn-in\nposterior_samples = samples[burn_in:]\nbayes_results={}\n# Posterior summaries\nfor i, name in enumerate(X_cols):\n    param_samples = posterior_samples[:, i]\n    mean = np.mean(param_samples)\n    std = np.std(param_samples)\n    ci_low, ci_high = np.percentile(param_samples, [2.5, 97.5])\n    print(f\"{name}: Beta = {mean:.4f}, SD = {std:.4f}, 95% CI = ({ci_low:.4f}, {ci_high:.4f})\")\n    bayes_results[name] = {\n        \"beta\": mean,\n        \"std\": std,\n        \"ci\": (ci_low, ci_high)\n    }\n```\n\n\nBelow is the trace plot of the algorithm, as well as the histogram of the posterior distribution for the price estimate.\n\n```{python}\nprint(posterior_samples)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming price is at index 3\nprice_samples = posterior_samples[:, 0]\n\n# Plot\nplt.figure(figsize=(12, 4))\n\n# Trace Plot\n# Trace Plot\nplt.plot(price_samples, color='blue', linewidth=0.7)\nplt.title(\"Trace Plot: Price Coefficient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"β (price)\")\nplt.show()\n\n# Posterior Histogram\nplt.hist(price_samples, bins=40, color='skyblue', edgecolor='black', density=True)\nplt.title(\"Posterior Distribution: Price Coefficient\")\nplt.xlabel(\"β (price)\")\nplt.ylabel(\"Density\")\nplt.show()\n```\nThe trace plot for the price coefficient shows stable oscillations around a central value, indicating that the Markov Chain Monte Carlo (MCMC) sampler has converged and is mixing well. There is no upward or downward drift, suggesting the chain is sampling effectively from the posterior distribution.\n\nThe corresponding posterior distribution is approximately normal, centered around -0.10. This confirms a strong negative effect of price on product choice — as price increases, the probability of selection decreases. The tight, symmetric shape of the distribution reflects high certainty in this estimate.\n\nThe following table summarizes the estimates from MLE and Bayesian methods.\n\n```{python}\n#| echo: false\nfrom IPython.display import Markdown\n\n# Build table header\ntable_md = \"| Parameter | MLE Estimate (SE) | 95% CI (MLE) | Bayes Mean (SD) | 95% Credible Interval |\\n\"\ntable_md += \"|-----------|-------------------|--------------|------------------|------------------------|\\n\"\n\n# Fill in rows dynamically\nfor param in ['price', 'brand_N', 'brand_P', 'ad_Yes']:\n    mle = mle_results[param]\n    bayes = bayes_results[param]\n    \n    row = (\n        f\"| **{param}** \"\n        f\"| {mle['beta']:.4f} ({mle['se']:.4f}) \"\n        f\"| ({mle['ci'][0]:.4f}, {mle['ci'][1]:.4f}) \"\n        f\"| {bayes['beta']:.4f} ({bayes['std']:.4f}) \"\n        f\"| ({bayes['ci'][0]:.4f}, {bayes['ci'][1]:.4f}) |\\n\"\n    )\n    table_md += row\n\n# Display the Markdown-formatted table\nMarkdown(table_md)\n```\n## 6. Discussion\n\nInterpretation of Parameter Estimates\nIf the data were not simulated, the parameter estimates would reflect real-world consumer preferences inferred from observed choices. In that case:\n\nA finding like $ \\beta_\\text{Netflix} > \\beta_\\text{Prime} $ means that, on average, consumers derive more utility from choosing Netflix compared to Amazon Prime, holding all other attributes constant.\n\nThis could indicate that consumers perceive higher value or satisfaction from Netflix’s offering (e.g., content library, user experience).\n\nIt also implies that, in the utility function a higher coefficient on Netflix leads to a higher probability of being chosen.\n\nRegarding price:\n\nA negative $\\beta_\\text{price}$ is expected and intuitive. It means that, all else equal, an increase in price decreases utility, which lowers the likelihood of the product being chosen.\n\nThis reflects basic economic theory: consumers prefer lower-cost options when utility from other features is equal.\n\n#### Multi-Level (Hierarchical) Model\n\nIn real-world conjoint analysis, consumer preferences are rarely homogeneous. The basic multinomial logit (MNL) model assumes that every individual shares the same set of preference parameters ($\\beta$), which is a strong and often unrealistic assumption.\n\nTo better reflect real-world heterogeneity in preferences, we use a **multi-level** model. Here's how this model can be both simulated and estimated:\n\n**1. Simulating Individual-Level Preferences**\n\nInstead of one global $\\beta$, we assume each respondent $i$ has their own parameter vector ${\\beta}_i$, drawn from a common population distribution:\n\n$$\n{\\beta}_i \\sim \\mathcal{N}({\\mu}, {\\Sigma})\n$$\n\n- ${\\mu}$: population mean of preferences  \n- ${\\Sigma}$: covariance matrix capturing variability and correlations between parameters  \n\nThis framework captures variation in individual tastes.\n\n---\n\n**2. Simulating Choice Data**\n\nUsing each individual's ${\\beta}_i$, simulate their choices for each task using the softmax choice probability:\n\n$$\nP_{ij} = \\frac{\\exp(\\mathbf{X}_{ij}^\\top {\\beta}_i)}{\\sum_k \\exp(\\mathbf{X}_{ik}^\\top {\\beta}_i)}\n$$\n\nWhere:\n- $j$ indexes alternatives\n- $\\mathbf{X}_{ij}$ are the attributes of alternative $j$\n- $P_{ij}$ is the probability that individual $i$ chooses alternative $j$\n\n---\n\n**3. Estimating the Model: Hierarchical Bayes**\n\nTo estimate a hierarchical model, use Hierarchical Bayesian (HB) methods such as:\n\n- Gibbs sampling  \n- Hamiltonian Monte Carlo \n- MCMC within Gibbs (as in traditional HB packages)\n\nThese methods estimate:\n\n- Individual-level coefficients $\\beta_i$\n- Population-level parameters $\\mu$, $\\Sigma$\n\nThis approach gives:\n- Personalized preference estimates\n- More realistic modeling of population behavior\n- Better predictive performance\n\n---\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"hw3_questions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"MLE and Conjoint Analysis","author":"Komal Nagaraj","date":"today"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}