[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Komal Nagaraj",
    "section": "",
    "text": "Welcome to my website! Professional Software Engineer with 3 years of experience in Python, SQL, Software Automation in Agile Software Development. Actively seeking opportunities in Data Analytics/ Data Science positions."
  },
  {
    "objectID": "projects/Does Price Matter in Charitable Giving/hw1_questions.html",
    "href": "projects/Does Price Matter in Charitable Giving/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse. Giving USA shows that in the United States, charitable gifts of money have been 2 percent or more of GDP since 1998, and more than 89 percent of Americans donate to charity. In order to understand the economics of charity, this field experiment was conducted to understand up to what extent the ‘price’ matters in a charitable fundraising using a mail solicitation. In other words the main aim of this experiment was to understand, can offering a matching grant (where a donation is “matched” by another donor) influence whether people give to charity—and how much they give?\nThe following experimental setup was used for the study:\nA large-scale field experiment was done with over 50,000 previous donors to a real liberal political charity in the U.S. The donors were randomly split into two groups: - Control group (no mention of a match) - Treatment group (offered a matching grant—e.g., if you donate $1, another $1, $2, or $3 is added)\nWithin the treatment group, three factors were varied: - Match ratio: $1:$1, $2:$1, or $3:$1 - Maximum match amount: $25,000, $50,000, $100,000, or unstated - Suggested donation amount: based on past giving (same as before, or slightly higher)\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#introduction",
    "href": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse. Giving USA shows that in the United States, charitable gifts of money have been 2 percent or more of GDP since 1998, and more than 89 percent of Americans donate to charity. In order to understand the economics of charity, this field experiment was conducted to understand up to what extent the ‘price’ matters in a charitable fundraising using a mail solicitation. In other words the main aim of this experiment was to understand, can offering a matching grant (where a donation is “matched” by another donor) influence whether people give to charity—and how much they give?\nThe following experimental setup was used for the study:\nA large-scale field experiment was done with over 50,000 previous donors to a real liberal political charity in the U.S. The donors were randomly split into two groups: - Control group (no mention of a match) - Treatment group (offered a matching grant—e.g., if you donate $1, another $1, $2, or $3 is added)\nWithin the treatment group, three factors were varied: - Match ratio: $1:$1, $2:$1, or $3:$1 - Maximum match amount: $25,000, $50,000, $100,000, or unstated - Suggested donation amount: based on past giving (same as before, or slightly higher)\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#data",
    "href": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nTo conduct this experiment 50,083 donors were chosen and different statistics were collected for this experiment. Individuals were randomly assigned to either a control group or a matching grant treatment group, and within the matching grant treatment group individuals were randomly assigned to different matching grant rates,matching grant maximum amounts, and suggested donation amounts.The collected data had the following information for each donor:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nThe following table describes the categorical data with the number of rows populated (Count), number of unique values in that particular field (unique), the most common value for a field (Top), and the number of occurrences of the top value (frequency).\n\n\n\n\n\n\nCategorical Data Summary\n\n\n\n\n\n\n\n\nVariable\nCount\nUnique\nTop\nFrequency\n\n\n\n\ntreatment\n50083\n2\n1\n33396\n\n\ncontrol\n50083\n2\n0\n33396\n\n\nratio\n50083\n4\nControl\n16687\n\n\nratio2\n50083\n2\n0\n38949\n\n\nratio3\n50083\n2\n0\n38954\n\n\nsize\n50083\n5\nControl\n16687\n\n\nsize25\n50083\n2\n0\n41733\n\n\nsize50\n50083\n2\n0\n41738\n\n\nsize100\n50083\n2\n0\n41733\n\n\nsizeno\n50083\n2\n0\n41732\n\n\nask\n50083\n4\nControl\n16687\n\n\naskd1\n50083\n2\n0\n38949\n\n\naskd2\n50083\n2\n0\n38950\n\n\naskd3\n50083\n2\n0\n38954\n\n\ngave\n50083\n2\n0\n49049\n\n\nltmedmra\n50083\n2\n0\n25356\n\n\nyear5\n50083\n2\n1\n25483\n\n\ndormant\n50083\n2\n1\n26217\n\n\nstate50one\n50083\n2\n0\n50033\n\n\nred0\n50048\n2\n0\n29806\n\n\nblue0\n50048\n2\n1\n29806\n\n\nredcty\n49978\n2\n1\n25501\n\n\nbluecty\n49978\n2\n0\n25553\n\n\n\n\n\n\nThe following tables have the number of rows populated for a field (Count), Mean value of the field (Mean), Standard deviation of the field (Std Dev), Minimum value of the field (Min), Maximum value of the field (Max) and 25, 50 and 75 percentile of the values for the field.\nSummary description on Numerical Variables:\n\n\n\n\n\n\nDonation and Donor Variables Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCount\nMean\nStd Dev\nMin\n25%\n50%\n75%\nMax\n\n\n\n\nask1\n50083\n71.50\n101.73\n25.00\n35.00\n45.00\n65.00\n1500.00\n\n\nask2\n50083\n91.79\n127.25\n35.00\n45.00\n60.00\n85.00\n1875.00\n\n\nask3\n50083\n111.05\n151.67\n50.00\n55.00\n70.00\n100.00\n2250.00\n\n\namount\n50083\n0.92\n8.71\n0.00\n0.00\n0.00\n0.00\n400.00\n\n\namountchange\n50083\n-52.67\n1267.10\n-200412.13\n-50.00\n-30.00\n-25.00\n275.00\n\n\nhpa\n50083\n59.38\n71.18\n0.00\n30.00\n45.00\n60.00\n1000.00\n\n\nfreq\n50083\n8.04\n11.39\n0.00\n2.00\n4.00\n10.00\n218.00\n\n\nyears\n50082\n6.10\n5.50\n0.00\n2.00\n5.00\n9.00\n95.00\n\n\nmrm2\n50082\n13.01\n12.08\n0.00\n4.00\n8.00\n19.00\n168.00\n\n\nfemale\n48972\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\ncouple\n48935\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\nDemographics and State-Level Variables Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCount\nMean\nStd Dev\nMin\n25%\n50%\n75%\nMax\n\n\n\n\nnonlit\n49631\n2.47\n1.96\n0.00\n1.00\n3.00\n4.00\n6.00\n\n\ncases\n49631\n1.50\n1.16\n0.00\n1.00\n1.00\n2.00\n4.00\n\n\nstatecnt\n50083\n6.00\n5.75\n0.002\n1.83\n3.54\n9.61\n17.37\n\n\nstateresponse\n50083\n0.0206\n0.0052\n0.00\n0.0182\n0.0197\n0.0230\n0.0769\n\n\nstateresponset\n50083\n0.0220\n0.0063\n0.00\n0.0185\n0.0217\n0.0247\n0.1111\n\n\nstateresponsec\n50080\n0.0177\n0.0075\n0.00\n0.0129\n0.0199\n0.0208\n0.0526\n\n\nstateresponsetminc\n50080\n0.0043\n0.0091\n-0.0476\n-0.0014\n0.0018\n0.0105\n0.1111\n\n\nperbush\n50048\n0.488\n0.079\n0.0909\n0.4444\n0.4848\n0.5253\n0.7320\n\n\nclose25\n50048\n0.186\n0.389\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\npwhite\n48217\n0.820\n0.169\n0.0094\n0.756\n0.873\n0.939\n1.00\n\n\npblack\n48047\n0.087\n0.136\n0.00\n0.0147\n0.0366\n0.0909\n0.9896\n\n\npage18_39\n48217\n0.322\n0.103\n0.00\n0.2583\n0.3055\n0.3691\n0.9975\n\n\nave_hh_sz\n48221\n2.43\n0.38\n0.00\n2.21\n2.44\n2.66\n5.27\n\n\nmedian_hhincome\n48209\n54815.70\n22027.32\n5000.00\n39181.00\n50673.00\n66005.00\n200001.00\n\n\npowner\n48214\n0.669\n0.193\n0.00\n0.560\n0.712\n0.817\n1.00\n\n\npsch_atlstba\n48215\n0.392\n0.187\n0.00\n0.236\n0.374\n0.530\n1.00\n\n\npop_propurban\n48217\n0.872\n0.259\n0.00\n0.885\n1.00\n1.00\n1.00\n\n\n\n\n\n\nFrom the above tables we can infer that -\n\nDonation behavior is highly skewed: while the average donation (amount) is $0.92, the median is $0, indicating that most donors did not give during the campaign.\nThe maximum recorded donation is $400, and the standard deviation is 8.71, showing wide variability among those who did give. The previous highest donation (hpa) had a mean of $59.38, with values ranging from $0 to $1000.\nDonor history shows that the average donor made 8 prior donations (freq), with some making over 200.\nState-level response rates averaged 2.06% overall, with slightly higher response among treated individuals (2.20%) compared to control (1.77%).\nDemographic indicators reveal:\n\nAbout 28% of donors were female, and 9% were couples.\nThe average median household income by zip code was around $54,816.\nThe average zip code was 82% white, 9% Black, and 87% urban.\nAbout 39% of zip code residents had a bachelor’s degree or higher.\n\n\n\n\nBalance Test\n\nT-statistic test on multiple variables in treatment and control groups\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another. t-test is performed on a number of variables among the treatment and control group. the following code is used to perform the t-tests on different variables.\n\nimport numpy as np\nfrom scipy.stats import t\n\n# Split treatment and control\ntreatment_group = df[df['treatment'] == 1]['amountchange'].dropna()\ncontrol_group = df[df['treatment'] == 0]['amountchange'].dropna()\n\n# Means\nmean_treat = np.mean(treatment_group)\nmean_control = np.mean(control_group)\n\n# Sizes\nn_treat = len(treatment_group)\nn_control = len(control_group)\n\n# Variances\nvar_treat = np.var(treatment_group, ddof=1)\nvar_control = np.var(control_group, ddof=1)\n\n# --- Manual t-statistic ---\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt((var_treat / n_treat) + (var_control / n_control))\nt_stat = numerator / denominator\n\n# --- Degrees of Freedom ---\ndf_num = (var_treat / n_treat + var_control / n_control) ** 2\ndf_den = ((var_treat / n_treat) ** 2) / (n_treat - 1) + ((var_control / n_control) ** 2) / (n_control - 1)\ndf = df_num / df_den\n\n# --- Two-tailed p-value ---\np_value = 2 * t.sf(np.abs(t_stat), df)\n\nprint(f\"T-statistic: {t_stat:.4f}\")\nprint(f\"Degrees of freedom: {df:.2f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nif p_value &lt; 0.05:\n    print(\"Statistically significant at 95% confidence level\")\nelse:\n    print(\"Not statistically significant\")\nThe following variables were tested (along with the t-test statistic):\n\n‘mrm2’ (number of months since last donation) - T-statistic: 0.1195 P-value: 0.9049. Inference: No statistically significant difference\n‘freq’ (Number of prior donations) - T-statistic: -0.1108 P-value: 0.9117. Inference: No statistically significant difference\n‘red0’ (Red state) - T-statistic: 1.8773 P-value: 0.0605. Inference - No statistically significant difference\n‘blue0’ (Blue state) - T-statistic: -1.8773 P-value: 0.0605. Inference - No statistically significant difference\n‘years’ (Number of years since initial donation) - T-statistic: -1.0909 P-value: 0.2753. Inference - No statistically significant difference\n‘hpa’ (Highest previous contribution) - T-statistic: 0.9704 P-value: 0.3318. Inference: No statistically significant difference\n‘psch_atlstba’ (Proportion who finished college within zip code) - T-statistic: -1.8427 P-value: 0.0654. Inference: No statistically significant difference\n‘amountchange’ (Change in amount given) - T-statistic: 0.4713 P-value: 0.6374. Inference: No statistically significant difference\n\n\n\nA regression analysis on ‘amountchange’\nA regression was conducted for ‘amountchange’ as dependent variable and ‘freq’, ‘years’, ‘hpa’,‘ask1’,‘ask2’,‘ask3’,‘mrm2’ as independent variable. Code shown below.\nimport statsmodels.api as sm\n\n# Prepare data\nX = df[[ 'freq', 'years', 'hpa','ask1','ask2','ask3','mrm2']]\ny = df['amountchange']\n\n# Add constant (intercept)\nX = sm.add_constant(X)\n\n# Fit linear regression model\nmodel = sm.OLS(y, X, missing='drop').fit()\n\n# Output results\nprint(model.summary())\n\nTable 1: Linear Regression Output – amountchange\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-Statistic\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nconst\n-24.914\n14.772\n-1.687\n0.092\n-53.867\n4.040\n\n\nfreq\n1.105\n0.675\n1.638\n0.102\n-0.218\n2.427\n\n\nyears\n-0.973\n1.405\n-0.693\n0.489\n-3.727\n1.781\n\n\nhpa\n-0.153\n0.218\n-0.703\n0.482\n-0.581\n0.274\n\n\nask1\n1.708\n5.187\n0.329\n0.742\n-8.458\n11.874\n\n\nask2\n1.246\n4.300\n0.290\n0.772\n-7.183\n9.674\n\n\nask3\n-2.361\n1.825\n-1.294\n0.196\n-5.939\n1.216\n\n\nmrm2\n0.319\n0.484\n0.659\n0.510\n-0.629\n1.267\n\n\n\n\nObservations: 50,082\n\nR-squared: 0.001\n\nAdjusted R-squared: 0.001\n]\n\nR-squared: 0.001 - Only 0.1% of the variation in amountchange is explained by the model — this is very low, which is expected in a clean RCT. All p-values &gt; 0.05 None of the pre-treatment variables are statistically significant predictors.\nFrom the above tests (regression and t-test), we see that none of the tested variables show a statistically significant difference between the treatment and control groups. This supports the validity of the experiment as a randomized controlled trial (RCT), where treatment assignment is expected to be independent of observable characteristics."
  },
  {
    "objectID": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#experimental-results",
    "href": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. The following chart shows the proportion of Donors in Control and Treatment group.\n\nFigure 1: Proportion of Donors by Group\n\n\n\nBarplot comparing donation rates\n\n\nA T-test on ‘gave’ yields the following statistical results -\n\n\n\nStat\nValue\n\n\n\n\nT-statistic\n3.2095\n\n\nP-value\n0.0013\n\n\n\nStatistically significant difference for ‘gave’ in treatment and control group (p &lt; 0.05).\nInference: There is a statistically significant difference in donation behavior between people who received the treatment (matching grant offer) and those who did not. From the bar plot above, we can say that people in the treatment group were more likely to donate, meaning the matching grant positively influenced giving behavior.\n\n\nTable 2: Probit Regression Output – Probability of Donation\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMarginal Effect\nStd. Error\nz-value\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\nFrom the above results we observe that being in a treatment group has a p-value&lt; 0.05 and hence, offering a match (treating the donors) results in an increase in the likelihood of a donor donating any amount of money.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nA series of T-tests are performed to test whether the size of the match ratio has an effect on whether people donate or not. The following observations were recorded :\n\nTable 3: T-Test Results for Match Ratio Comparisons\n\n\n\nTest Comparison\nt-statistic\np-value\n\n\n\n\n1:1 vs 2:1\n-0.9650\n0.3345\n\n\n1:1 vs 3:1\n-1.0150\n0.3101\n\n\n2:1 vs 3:1\n-0.0501\n0.9600\n\n\n\nAll the above tests have a very large P-value (p &gt; 0.05). This suggests that there is no significant statistical difference in the donation likelihood when different higher match ratios are offered which are in line with the author’s article on page 8.\nA regression test to support the above argument is also conducted with match ratios as the independent variable and ‘gave’ as the dependent variable. The code below is used to regress ‘gave’ on match ratios code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('file_path')\n\n# Create dummy for ratio1 (1:1 match)\ndf['ratio1'] = np.where(df['ratio'] == \"1\", 1, 0)\n\n# Prepare features and outcome\nX = df[['ratio1', 'ratio2', 'ratio3']]\ny = df['gave'].astype(int)\n\n# Add constant\nX = sm.add_constant(X)\n\n# Fit OLS model\nmodel = sm.OLS(y, X, missing='drop').fit()\n\n# Show summary\nprint(model.summary())\nThe results of the regression are tabulated further. #### Table 4: OLS Regression — Probability of Donation (gave)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-Statistic\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nconst\n0.0179\n0.001\n16.225\n0.000\n0.016\n0.020\n\n\nratio1\n0.0029\n0.002\n1.661\n0.097\n-0.001\n0.006\n\n\nratio2\n0.0048\n0.002\n2.744\n0.006\n0.001\n0.008\n\n\nratio3\n0.0049\n0.002\n2.802\n0.005\n0.001\n0.008\n\n\n\nAgain, it is observed from Table 4 that, none of the match ratio variables are statistically significant in influencing the likelihood of a donor making a donation. Both the T-tests and the Regression results support this argument.\nThe difference in average response rate between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios are calculated from the data collected. The values are shown in Table 5.2 The difference in average response rate between thecoeffecients of match ratios calculated using the regression (as seen in Table 4) estimated earlier turns out to be equal to those seen in Table 5.2 and Table 6.\n# Calculate group-wise response rates from raw data\nrate_1to1 = df[df['ratio1'] == 1]['gave'].mean()\nrate_2to1 = df[df['ratio2'] == 1]['gave'].mean()\nrate_3to1 = df[df['ratio3'] == 1]['gave'].mean()\n\n# Compute differences\ndiff_2_vs_1 = rate_2to1 - rate_1to1\ndiff_3_vs_2 = rate_3to1 - rate_2to1\ndiff_3_vs_1 = rate_3to1 - rate_1to1\n\n\nTable 5.1: Response Rates by Match Ratio (From Data)\n\n\n\nMatch Ratio\nResponse Rate\n\n\n\n\n1:1\n0.0207\n\n\n2:1\n0.0226\n\n\n3:1\n0.0227\n\n\n\n\n\nTable 5.2: Response Rate Differences by Match Ratio (From Data)\n\n\n\nComparison\nResponse Rate Difference\n\n\n\n\n2:1 vs 1:1\n0.0019\n\n\n3:1 vs 2:1\n0.0001\n\n\n3:1 vs 1:1\n0.0020\n\n\n\n\n\nTable 6: Regression Coefficient Differences by Match Ratio\n\n\n\nComparison\nRegression Coefficient Difference\n\n\n\n\n2:1 vs 1:1\n0.0048 - 0.0029 = 0.0019\n\n\n3:1 vs 2:1\n0.0049 - 0.0048 = 0.0001\n\n\n3:1 vs 1:1\n0.0049 - 0.0029 = 0.0020\n\n\n\nBoth the raw data and the regression results show that increasing the match ratio has only a very small effect on the probability of donating. The increase from a 1:1 to a 2:1 match raises the response rate by just 0.19 percentage points, and from 2:1 to 3:1 by only 0.01 percentage points.This indicates that larger match ratios do not provide additional meaningful gains, consistent with the authors’ conclusion.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI performed a T-test on the ‘amount’ variable for treatment group.\n\nimport numpy as np\nfrom scipy.stats import t\n\n# Prepare data\ntreat_amt = df[df['treatment'] == 1]['amount'].dropna()\ncontrol_amt = df[df['treatment'] == 0]['amount'].dropna()\n\n# Sample sizes\nn1 = len(treat_amt)\nn0 = len(control_amt)\n\n# Means\nmean1 = np.mean(treat_amt)\nmean0 = np.mean(control_amt)\n\n# Variances (sample)\nvar1 = np.var(treat_amt, ddof=1)\nvar0 = np.var(control_amt, ddof=1)\n\n# Compute t-statistic manually\nnumerator = mean1 - mean0\ndenominator = np.sqrt((var1 / n1) + (var0 / n0))\nt_stat_manual = numerator / denominator\n\n# Degrees of freedom \ndf_num = (var1 / n1 + var0 / n0) ** 2\ndf_den = ((var1 / n1) ** 2) / (n1 - 1) + ((var0 / n0) ** 2) / (n0 - 1)\ndof = df_num / df_den\n\n# Compute p-value (two-sided)\np_val_manual = 2 * t.sf(np.abs(t_stat_manual), dof)\nThe t-statistic was 1.9183 and the p-value was 0.0551, which is slightly above the conventional 5% significance level. This indicates that the difference in average donation amount between the treatment and control groups is not statistically significant at the standard threshold. While the result suggests that receiving a matching grant letter might increase donation amounts, the evidence is not strong enough to confirm a significant effect.\nTo analyze the effect of treatment on donation amount conditional on donating, I ran a bivariate OLS regression restricted to individuals with positive donation amounts (amount &gt; 0). The tabulated in Table 7. The intercept indicates that control group donors donated an average of $45.54, while treated donors donated $1.67 less on average. However, this difference is not statistically significant since the p value = 0.561). Since the model conditions on having donated, the treatment effect here does not have a causal interpretation, and reflects only differences among those already inclined to give.\n\nTable 7: OLS Regression — Donation Amount (Only Donors)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-Statistic\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nconst\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n–1.6684\n2.872\n–0.581\n0.561\n–7.305\n3.968\n\n\n\n\nNumber of observations: 1,034\n\nR-squared: 0.000\n\nF-statistic: 0.3374 (p = 0.561)\n\nDependent variable: amount (only where amount &gt; 0)\n\nWe plot histograms for the treatment group and one for the control donation amounts only among people who donated. The code below is used to generate these plots. The plot obtain is seen below in Figure 2.\n#| label: donation-histograms\n#| fig-cap: \"Histogram of Donation Amounts for Treatment and Control Groups (Donors Only)\"\n#| fig-subcap: [\"Treatment Group\", \"Control Group\"]\n#| layout-ncol: 2\n#| echo: true\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to donors only\ndonors = df[df['amount'] &gt; 0].copy()\n\n# Split by group\ntreat_donors = donors[donors['treatment'] == 1]\ncontrol_donors = donors[donors['treatment'] == 0]\n\n# Calculate means\nmean_treat = treat_donors['amount'].mean()\nmean_control = control_donors['amount'].mean()\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\nfig.suptitle(\"Histogram of Donation Amounts for Treatment and Control Groups (Donors Only)\", fontsize=16)\n# Treatment histogram\nsns.histplot(treat_donors['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f'Mean = {mean_treat:.2f}')\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Control histogram\nsns.histplot(control_donors['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f'Mean = {mean_control:.2f}')\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 2: Histogram of Dantion Amounts for Treatment and Control Groups\n Both groups have a highly right-skewed distribution — most donors give smaller amounts, but a few give much more. The average donation in the control group is slightly higher than in the treatment group. This matches your earlier OLS regression result: The treatment group gave $1.67 less on average (not statistically significant)."
  },
  {
    "objectID": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#simulation-experiment",
    "href": "projects/Does Price Matter in Charitable Giving/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. This average will likely be “noisey” when only averaging a few numbers, but should “settle down” and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\nI simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution and further calculate a vector of 10,000 differences. This simulation in conducted using the code below\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 binary draws\ncontrol = np.random.binomial(n=1, p=0.018, size=10000)\ntreatment = np.random.binomial(n=1, p=0.022, size=10000)\n\n# Compute difference in outcomes\ndifferences = treatment - control\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, color='purple')\nplt.axhline(0.004, color='red', linestyle='--', label='True Treatment Effect = 0.004')\nplt.title(\"Cumulative Average of Simulated Differences Between Treatment and Control\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nFigure 3: Cumulative Average of Simulated Differences Between Treatment and Control\n The chart above demonstrates the Law of Large Numbers (LLN) using simulated binary outcomes from a Bernoulli distribution. In this simulation:\n\nControl group donations were drawn with probability p = 0.018\nTreatment group donations were drawn with p = 0.022\n\nThe cumulative difference in donation behavior between the groups was tracked across 10,000 simulations. While the average difference fluctuates at first due to randomness, it gradually settles near 0.004, the true treatment effect. This convergence illustrates the Law of Large Numbers: as the sample size increases, the sample average approaches the true population average.\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader.\n\n\nCentral Limit Theorem: Simulation\nThe sequence of histograms below in Figure 4 illustrates the Central Limit Theorem (CLT). Each plot shows the distribution of the difference in sample means between a treatment group (p = 0.022) and a control group (p = 0.018) based on 1000 simulations for different sample sizes.\nAs the sample size increases: - The distribution of the average differences becomes more symmetric and bell-shaped - The sampling distribution centers around the true treatment effect (0.004) - The variability (spread) decreases\nThis demonstrates the CLT: as sample size increases, the sampling distribution of the sample mean approaches a normal distribution, regardless of the original (Bernoulli) distribution of the data. The code for simulation is given below.\n\nFigure 4: Central Limit Theorem - Simulation\n\n\n\nCentral Limit Theorem - Simulation\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Settings\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000\ntrue_diff = 0.022 - 0.018\n\n# Create 4 subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\nfor idx, n in enumerate(sample_sizes):\n    diffs = []\n\n    for _ in range(simulations):\n        control = np.random.binomial(1, 0.018, size=n)\n        treatment = np.random.binomial(1, 0.022, size=n)\n        diff = np.mean(treatment) - np.mean(control)\n        diffs.append(diff)\n\n    ax = axes[idx // 2][idx % 2]\n    sns.histplot(diffs, bins=30, kde=True, ax=ax, color='lightblue')\n    ax.axvline(true_diff, color='red', linestyle='--', label='True Treatment Effect = 0.004')\n    ax.set_title(f\"Sample Size = {n}\")\n    ax.set_xlabel(\"Difference in Means\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nKomal Nagaraj\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMLE and COnjoint Analysis\n\n\n\n\n\n\nKomal Nagaraj\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nKomal Nagaraj Kattigenahally\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/MLE/hw2_questions.html",
    "href": "projects/MLE/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nLet’s read the data for Blueprinty’s\n\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nblueprint = pd.read_csv('blueprinty.csv')\nblueprint.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nmean_iscust = blueprint[blueprint['iscustomer'] == 1]['patents'].mean()\nmean_isnot_cust = blueprint[blueprint['iscustomer'] == 0]['patents'].mean()\nprint(\"Mean number of patents for Blueprinty's customers: \", round(mean_iscust, 3))\nprint(\"Mean number of patents of non-Blueprinty's customers: \", round(mean_isnot_cust, 2))\n\n# Histogram of number of patents for Blueprinty customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 1]['patents'], bins=20, color='blue', alpha=0.7)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents - Blueprint Customers')\nplt.grid(True)\nplt.show()\n\n# Histogram of number of patents for non-customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 0]['patents'], bins=20, color='orange', alpha=0.7)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title(\"Histogram of Patents - Non-Customers\")\nplt.grid(True)\nplt.show()\n\nMean number of patents for Blueprinty's customers:  4.133\nMean number of patents of non-Blueprinty's customers:  3.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above histograms of number of patents by customer status it is observed that, for non customers the highest number of patents are around 2-3 and for Blueprinty’s customers it is around 4-5. The mean number of patents for Blueprinty’s customers is 4.133 and for non customters it is 3.47. A larger proportion of Blueprinty’s customers have large number of patents (8-16) as compared to that of non-customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet’s vizualize the age distribution of customers vs non-customers\n\n# Plot for customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 1]['age'], bins=20, color='purple', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution - Customers')\nplt.grid(True)\nplt.show()\n\n# Plot for non-customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 0]['age'], bins=20, color='skyblue', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution - Non-Customers')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above age distributions of Blueprinty’s current customers it is seen that -\n\nThe distribution is spread out, but heavily skewed toward younger ages (18–30)\nDue to its concentration in younger age group this suggests Blueprint is more attractive to younger individuals or startups.\n\nThe non-customer age distribution suggests the following -\n\nIt is more concentrated, suggesting a larger and more consistent population.\nThe age range is broader and more evenly distributed. This suggests the non-customer segment includes more mature, possibly more established individuals or companies.\n\nLet’s visualize the distribution by region for customers vs non-customers\n\n# Visualize regional distribution by customer status\n\n# Histogram for customers\nplt.figure(figsize=(8, 5))\ncustomer_regions = blueprint[blueprint['iscustomer'] == 1]['region']\nplt.hist(customer_regions, bins=20, color='violet', alpha=0.7)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Regional Distribution - Customers')\nplt.grid(True)\nplt.show()\n\n# Histogram for non-customers\nplt.figure(figsize=(8, 5))\nnon_customer_regions = blueprint[blueprint['iscustomer'] == 0]['region']\nplt.hist(non_customer_regions, bins=20, color='lightgreen', alpha=0.7)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Regional Distribution - Non-Customers')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the Customers of Blueprinty:\n\nNortheast has the largest number of customers — over 300 customers.\nAll other regions (Southwest, Midwest, South, Northwest) have much smaller numbers — around 30–50 each.\nThis suggests Blueprint is very strong in the Northeast and underrepresented elsewhere.\n\nFor Non-Customers: - The distribution is much more balanced distribution across all regions. - Northeast, Southwest, Midwest lead, each with ~250–270. - Northwest and South are slightly lower, but still significant (~150+).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\)\nThe likelihood function for the entire sample is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nNow we will use Python to estimate the log likelihood function. Code below :\n\nimport numpy as np\nfrom math import factorial\n\n# Poisson likelihood function\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # avoid log(0)\n    return -len(Y)*lmbda + np.sum(Y)*np.log(lmbda) - np.sum(np.log([factorial(y) for y in Y]))\n\nNext, we will plot the Poisson Log-Likelihood vs Lambda. This plot visualizes how the Poisson log-likelihood function changes as we vary the rate parameter λ (lambda) — which represents the expected count of patents. Some points to note in the plot -\n\nWhen λ is too low or too high, the log-likelihood drops, meaning those values poorly explain the data.\nThe maximum point on the curve gives the λ that best fits the data — this is the MLE, and is often equal to the sample mean in a simple Poisson model.\nThe curve helps us visualize parameter uncertainty: a flatter curve means more uncertainty in the estimate; a steeper curve means the estimate is more precise.\n\n\nY = blueprint['patents']\nlambda_vals = np.linspace(0.1, 10, 100)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Find MLE\nmle_lambda = lambda_vals[np.argmax(loglik_vals)]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, label=\"Log-Likelihood\", color='blue')\nplt.axvline(x=mle_lambda, color='red', linestyle='--', label=f'MLE = {mle_lambda:.2f}')\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow, we will estimate the MLE by making use of the first derivative of log-likelhood function. We will do this with Python :\n\ndef poisson_loglikelihood_derivative(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.nan  # avoid division by zero\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    return -n + (sum_Y / lmbda)\n\n# Plot log-likelihood and its derivative\nlambda_vals = np.linspace(0.1, 10, 100)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\nderivative_vals = [poisson_loglikelihood_derivative(lmbda, Y) for lmbda in lambda_vals]\n\n# Find MLE\nmle_lambda = lambda_vals[np.argmax(loglik_vals)]\n\n\n# Plot derivative\nplt.plot(lambda_vals, derivative_vals, label=\"First Derivative\", color='green')\nplt.axhline(y=0, color='black', linestyle='--')\nplt.axvline(x=mle_lambda, color='red', linestyle='--', label=\"Zero Crossing (MLE)\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Derivative\")\nplt.title(\"First Derivative of Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFurther, we will estimate the MLE by optimizing the likelihood function with sp.optimize() in Python\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom math import factorial, log\n\n# Sample observed data\nY = blueprint['patents']\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda_array):\n    lmbda = lmbda_array[0]\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid values\n    n = len(Y)\n    sum_Y = sum(Y)\n    const_term = sum([log(factorial(y)) for y in Y])  # optional\n    return -(-n * lmbda + sum_Y * log(lmbda) - const_term)\n\n# Initial guess\ninitial_lambda = [1.0]\n\n# Minimize\nresult = minimize(neg_log_likelihood, initial_lambda, method='L-BFGS-B', bounds=[(0.0001, None)])\n\n# Extract MLE\nlambda_mle = result.x[0]\nprint(f\"MLE for λ using minimize(): {lambda_mle:.4f}\")\n\nMLE for λ using minimize(): 3.6847\n\n\nThree approaches are used to find the MLE value. the results from all three approaches are as follows :\n\nGrid Search and Plotting: The log-likelihood is computed over a grid of λ values and plotted them. The maximum occurred at approximatel 3.68.\nFirst Derivative : The first derivate is taken and plotted. The derivative crossed zero at the approximately same λ, confirming the MLE analytically.\n\nNumerical Optimization: Using scipy.optimize.minimize(), we minimized the negative log-likelihood and MLE obtained is 3.6847\nThis matched our previous results closely, validating both the numerical and analytical solutions.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\n\n# Poisson log-likelihood for regression model\ndef poisson_log_likelihood(beta, X, Y):\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n    beta = np.asarray(beta)\n\n    # Linear predictor: Xβ\n    eta = X @ beta\n\n    # Inverse link function: λ = exp(η)\n    lam = np.exp(eta)\n\n    # Log-likelihood\n    loglik = np.sum(Y * np.log(lam) - lam - np.log([np.math.factorial(int(y)) for y in Y]))\n\n    return -loglik  # negative for use with minimize()\n\nNow we use Python’s sp.optimze() to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom scipy.special import gammaln\n\n# ---- Feature Engineering ----\nblueprint['age_sq'] = blueprint['age'] ** 2\nregion_dummies = pd.get_dummies(blueprint['region'], drop_first=True)\n\n# ---- Design Matrix X ----\nX = pd.concat([\n    pd.Series(1, index=blueprint.index, name='Intercept'),\n    blueprint[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\nX_matrix =  X.astype(float).values\n\n# ---- Outcome Variable Y ----\nY = blueprint['patents'].values\n\n# ---- Poisson Log-Likelihood Function ----\ndef poisson_log_likelihood(beta, X, Y):\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # safe range for exp\n    lam = np.exp(Xb)\n    loglik = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n    return -loglik  # negative for minimization\n\n# ---- Estimation ----\nbeta_init = np.zeros(X_matrix.shape[1])\nresult = minimize(poisson_log_likelihood, beta_init, args=(X_matrix, Y), method='BFGS')\n\n# ---- Extract Coefficients and Standard Errors ----\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# ---- Results Table ----\nresults_df = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': std_errors\n}, index=X.columns)\n\nprint(results_df.round(4))\n\n            Coefficient  Std. Error\nIntercept       -0.5100      0.1931\nage              0.1487      0.0145\nage_sq          -0.0030      0.0003\niscustomer       0.2076      0.0329\nNortheast        0.0292      0.0468\nNorthwest       -0.0176      0.0572\nSouth            0.0566      0.0562\nSouthwest        0.0506      0.0496\n\n\nChecking for similar results using Python’s sm.GLM() below:\n\nimport statsmodels.api as sm\n\n# Make sure X and Y are both purely numeric\nX_numeric = X.astype(float)\nY_numeric = pd.Series(Y).astype(float)\n\n# Fit GLM model\nmodel = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\nresults = model.fit()\n\n# Print the results\nprint(results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:32:46   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n\n\nInterpretation\nWe estimated a Poisson regression model to examine the relationship between firm characteristics and the number of patents awarded. The following interpretations can be made based on the results obatined -\n\nAge: The coefficient for age is positive and statistically significant (p &lt; 0.001), while the coefficient for age_sq is negative and also highly significant. This suggests a nonlinear relationship between firm age and patenting activity. Specifically, patenting increases with age up to a point, then begins to decline — forming an inverted-U relationship\nCustomer Status: The variable iscustomer has a positive and significant coefficient (p &lt; 0.001), indicating that firms who are Blueprinty customers file approximately 23% more patents than non-customers, all else equal. This highlights a potential link between Blueprint’s services and increased innovation outcomes.\nRegion: Coefficients for regional indicators (Northeast, Northwest, South, Southwest) are not statistically significant at the 5% level. This implies that, after controlling for age and customer status, regional differences in patenting are minimal or not detectable in this sample.\nThe pseudo R-squared is 0.136, indicating a modest but meaningful improvement over a null model.\n\nNow we estimate the number of patents for customers vs non-customers using the following mthods : We create X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and the fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\nimport numpy as np\n\n# Create two counterfactual datasets:\n# X_0: everyone is NOT a customer (iscustomer = 0)\n# X_1: everyone IS a customer (iscustomer = 1)\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\n# Convert to matrix form\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted number of patents using fitted model\nXb_0 = np.clip(X_0_matrix @ beta_hat, -20, 20)\nXb_1 = np.clip(X_1_matrix @ beta_hat, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)\ny_pred_1 = np.exp(Xb_1)\n\n# Difference in predicted patent counts\ndelta = y_pred_1 - y_pred_0\naverage_diff = np.mean(delta)\n\nprint(\"Average difference in number of patents for customers vs non customers:\" , round(average_diff,3))\n\nAverage difference in number of patents for customers vs non customers: 0.793\n\n\nThe above results imply that Firms using Blueprinty’s software are predicted to produce, on average, 0.79 more patents than they would have without it - all else held constant."
  },
  {
    "objectID": "projects/MLE/hw2_questions.html#blueprinty-case-study",
    "href": "projects/MLE/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nLet’s read the data for Blueprinty’s\n\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nblueprint = pd.read_csv('blueprinty.csv')\nblueprint.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nmean_iscust = blueprint[blueprint['iscustomer'] == 1]['patents'].mean()\nmean_isnot_cust = blueprint[blueprint['iscustomer'] == 0]['patents'].mean()\nprint(\"Mean number of patents for Blueprinty's customers: \", round(mean_iscust, 3))\nprint(\"Mean number of patents of non-Blueprinty's customers: \", round(mean_isnot_cust, 2))\n\n# Histogram of number of patents for Blueprinty customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 1]['patents'], bins=20, color='blue', alpha=0.7)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents - Blueprint Customers')\nplt.grid(True)\nplt.show()\n\n# Histogram of number of patents for non-customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 0]['patents'], bins=20, color='orange', alpha=0.7)\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title(\"Histogram of Patents - Non-Customers\")\nplt.grid(True)\nplt.show()\n\nMean number of patents for Blueprinty's customers:  4.133\nMean number of patents of non-Blueprinty's customers:  3.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above histograms of number of patents by customer status it is observed that, for non customers the highest number of patents are around 2-3 and for Blueprinty’s customers it is around 4-5. The mean number of patents for Blueprinty’s customers is 4.133 and for non customters it is 3.47. A larger proportion of Blueprinty’s customers have large number of patents (8-16) as compared to that of non-customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet’s vizualize the age distribution of customers vs non-customers\n\n# Plot for customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 1]['age'], bins=20, color='purple', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution - Customers')\nplt.grid(True)\nplt.show()\n\n# Plot for non-customers\nplt.figure(figsize=(8, 5))\nplt.hist(blueprint[blueprint['iscustomer'] == 0]['age'], bins=20, color='skyblue', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution - Non-Customers')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above age distributions of Blueprinty’s current customers it is seen that -\n\nThe distribution is spread out, but heavily skewed toward younger ages (18–30)\nDue to its concentration in younger age group this suggests Blueprint is more attractive to younger individuals or startups.\n\nThe non-customer age distribution suggests the following -\n\nIt is more concentrated, suggesting a larger and more consistent population.\nThe age range is broader and more evenly distributed. This suggests the non-customer segment includes more mature, possibly more established individuals or companies.\n\nLet’s visualize the distribution by region for customers vs non-customers\n\n# Visualize regional distribution by customer status\n\n# Histogram for customers\nplt.figure(figsize=(8, 5))\ncustomer_regions = blueprint[blueprint['iscustomer'] == 1]['region']\nplt.hist(customer_regions, bins=20, color='violet', alpha=0.7)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Regional Distribution - Customers')\nplt.grid(True)\nplt.show()\n\n# Histogram for non-customers\nplt.figure(figsize=(8, 5))\nnon_customer_regions = blueprint[blueprint['iscustomer'] == 0]['region']\nplt.hist(non_customer_regions, bins=20, color='lightgreen', alpha=0.7)\nplt.xlabel('Region')\nplt.ylabel('Frequency')\nplt.title('Regional Distribution - Non-Customers')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the Customers of Blueprinty:\n\nNortheast has the largest number of customers — over 300 customers.\nAll other regions (Southwest, Midwest, South, Northwest) have much smaller numbers — around 30–50 each.\nThis suggests Blueprint is very strong in the Northeast and underrepresented elsewhere.\n\nFor Non-Customers: - The distribution is much more balanced distribution across all regions. - Northeast, Southwest, Midwest lead, each with ~250–270. - Northwest and South are slightly lower, but still significant (~150+).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\)\nThe likelihood function for the entire sample is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\]\nNow we will use Python to estimate the log likelihood function. Code below :\n\nimport numpy as np\nfrom math import factorial\n\n# Poisson likelihood function\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # avoid log(0)\n    return -len(Y)*lmbda + np.sum(Y)*np.log(lmbda) - np.sum(np.log([factorial(y) for y in Y]))\n\nNext, we will plot the Poisson Log-Likelihood vs Lambda. This plot visualizes how the Poisson log-likelihood function changes as we vary the rate parameter λ (lambda) — which represents the expected count of patents. Some points to note in the plot -\n\nWhen λ is too low or too high, the log-likelihood drops, meaning those values poorly explain the data.\nThe maximum point on the curve gives the λ that best fits the data — this is the MLE, and is often equal to the sample mean in a simple Poisson model.\nThe curve helps us visualize parameter uncertainty: a flatter curve means more uncertainty in the estimate; a steeper curve means the estimate is more precise.\n\n\nY = blueprint['patents']\nlambda_vals = np.linspace(0.1, 10, 100)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Find MLE\nmle_lambda = lambda_vals[np.argmax(loglik_vals)]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, label=\"Log-Likelihood\", color='blue')\nplt.axvline(x=mle_lambda, color='red', linestyle='--', label=f'MLE = {mle_lambda:.2f}')\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow, we will estimate the MLE by making use of the first derivative of log-likelhood function. We will do this with Python :\n\ndef poisson_loglikelihood_derivative(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.nan  # avoid division by zero\n    n = len(Y)\n    sum_Y = np.sum(Y)\n    return -n + (sum_Y / lmbda)\n\n# Plot log-likelihood and its derivative\nlambda_vals = np.linspace(0.1, 10, 100)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\nderivative_vals = [poisson_loglikelihood_derivative(lmbda, Y) for lmbda in lambda_vals]\n\n# Find MLE\nmle_lambda = lambda_vals[np.argmax(loglik_vals)]\n\n\n# Plot derivative\nplt.plot(lambda_vals, derivative_vals, label=\"First Derivative\", color='green')\nplt.axhline(y=0, color='black', linestyle='--')\nplt.axvline(x=mle_lambda, color='red', linestyle='--', label=\"Zero Crossing (MLE)\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Derivative\")\nplt.title(\"First Derivative of Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFurther, we will estimate the MLE by optimizing the likelihood function with sp.optimize() in Python\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom math import factorial, log\n\n# Sample observed data\nY = blueprint['patents']\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda_array):\n    lmbda = lmbda_array[0]\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid values\n    n = len(Y)\n    sum_Y = sum(Y)\n    const_term = sum([log(factorial(y)) for y in Y])  # optional\n    return -(-n * lmbda + sum_Y * log(lmbda) - const_term)\n\n# Initial guess\ninitial_lambda = [1.0]\n\n# Minimize\nresult = minimize(neg_log_likelihood, initial_lambda, method='L-BFGS-B', bounds=[(0.0001, None)])\n\n# Extract MLE\nlambda_mle = result.x[0]\nprint(f\"MLE for λ using minimize(): {lambda_mle:.4f}\")\n\nMLE for λ using minimize(): 3.6847\n\n\nThree approaches are used to find the MLE value. the results from all three approaches are as follows :\n\nGrid Search and Plotting: The log-likelihood is computed over a grid of λ values and plotted them. The maximum occurred at approximatel 3.68.\nFirst Derivative : The first derivate is taken and plotted. The derivative crossed zero at the approximately same λ, confirming the MLE analytically.\n\nNumerical Optimization: Using scipy.optimize.minimize(), we minimized the negative log-likelihood and MLE obtained is 3.6847\nThis matched our previous results closely, validating both the numerical and analytical solutions.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\n\n# Poisson log-likelihood for regression model\ndef poisson_log_likelihood(beta, X, Y):\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n    beta = np.asarray(beta)\n\n    # Linear predictor: Xβ\n    eta = X @ beta\n\n    # Inverse link function: λ = exp(η)\n    lam = np.exp(eta)\n\n    # Log-likelihood\n    loglik = np.sum(Y * np.log(lam) - lam - np.log([np.math.factorial(int(y)) for y in Y]))\n\n    return -loglik  # negative for use with minimize()\n\nNow we use Python’s sp.optimze() to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom scipy.special import gammaln\n\n# ---- Feature Engineering ----\nblueprint['age_sq'] = blueprint['age'] ** 2\nregion_dummies = pd.get_dummies(blueprint['region'], drop_first=True)\n\n# ---- Design Matrix X ----\nX = pd.concat([\n    pd.Series(1, index=blueprint.index, name='Intercept'),\n    blueprint[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\nX_matrix =  X.astype(float).values\n\n# ---- Outcome Variable Y ----\nY = blueprint['patents'].values\n\n# ---- Poisson Log-Likelihood Function ----\ndef poisson_log_likelihood(beta, X, Y):\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # safe range for exp\n    lam = np.exp(Xb)\n    loglik = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n    return -loglik  # negative for minimization\n\n# ---- Estimation ----\nbeta_init = np.zeros(X_matrix.shape[1])\nresult = minimize(poisson_log_likelihood, beta_init, args=(X_matrix, Y), method='BFGS')\n\n# ---- Extract Coefficients and Standard Errors ----\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# ---- Results Table ----\nresults_df = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': std_errors\n}, index=X.columns)\n\nprint(results_df.round(4))\n\n            Coefficient  Std. Error\nIntercept       -0.5100      0.1931\nage              0.1487      0.0145\nage_sq          -0.0030      0.0003\niscustomer       0.2076      0.0329\nNortheast        0.0292      0.0468\nNorthwest       -0.0176      0.0572\nSouth            0.0566      0.0562\nSouthwest        0.0506      0.0496\n\n\nChecking for similar results using Python’s sm.GLM() below:\n\nimport statsmodels.api as sm\n\n# Make sure X and Y are both purely numeric\nX_numeric = X.astype(float)\nY_numeric = pd.Series(Y).astype(float)\n\n# Fit GLM model\nmodel = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\nresults = model.fit()\n\n# Print the results\nprint(results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:32:46   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n\n\nInterpretation\nWe estimated a Poisson regression model to examine the relationship between firm characteristics and the number of patents awarded. The following interpretations can be made based on the results obatined -\n\nAge: The coefficient for age is positive and statistically significant (p &lt; 0.001), while the coefficient for age_sq is negative and also highly significant. This suggests a nonlinear relationship between firm age and patenting activity. Specifically, patenting increases with age up to a point, then begins to decline — forming an inverted-U relationship\nCustomer Status: The variable iscustomer has a positive and significant coefficient (p &lt; 0.001), indicating that firms who are Blueprinty customers file approximately 23% more patents than non-customers, all else equal. This highlights a potential link between Blueprint’s services and increased innovation outcomes.\nRegion: Coefficients for regional indicators (Northeast, Northwest, South, Southwest) are not statistically significant at the 5% level. This implies that, after controlling for age and customer status, regional differences in patenting are minimal or not detectable in this sample.\nThe pseudo R-squared is 0.136, indicating a modest but meaningful improvement over a null model.\n\nNow we estimate the number of patents for customers vs non-customers using the following mthods : We create X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and the fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\nimport numpy as np\n\n# Create two counterfactual datasets:\n# X_0: everyone is NOT a customer (iscustomer = 0)\n# X_1: everyone IS a customer (iscustomer = 1)\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\n# Convert to matrix form\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted number of patents using fitted model\nXb_0 = np.clip(X_0_matrix @ beta_hat, -20, 20)\nXb_1 = np.clip(X_1_matrix @ beta_hat, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)\ny_pred_1 = np.exp(Xb_1)\n\n# Difference in predicted patent counts\ndelta = y_pred_1 - y_pred_0\naverage_diff = np.mean(delta)\n\nprint(\"Average difference in number of patents for customers vs non customers:\" , round(average_diff,3))\n\nAverage difference in number of patents for customers vs non customers: 0.793\n\n\nThe above results imply that Firms using Blueprinty’s software are predicted to produce, on average, 0.79 more patents than they would have without it - all else held constant."
  },
  {
    "objectID": "projects/MLE/hw2_questions.html#airbnb-case-study",
    "href": "projects/MLE/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Load the Airbnb data (change path if needed)\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# --- Clean and prepare data ---\nairbnb_clean = airbnb[[\n    'room_type', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]].copy()\n\n# Convert price to float\nairbnb_clean['price'] = (\n    airbnb_clean['price'].replace('[\\$,]', '', regex=True).astype(float)\n)\n\n# Drop rows with missing values\nairbnb_clean.dropna(inplace=True)\n\n# Convert categorical variables to dummy variables\nairbnb_encoded = pd.get_dummies(\n    airbnb_clean,\n    columns=['room_type', 'instant_bookable'],\n    drop_first=True  # avoids dummy variable trap\n)\n\n# --- Set up model inputs ---\nX = sm.add_constant(X)\nX = X.astype(float)\nY = Y.astype(float) \n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\nresults = poisson_model.fit()\n\n# --- Print model results ---\nsummary_df = pd.DataFrame({\n    'Coefficient': results.params,\n    'Std. Error': results.bse,\n    'z-value': results.tvalues,\n    'p-value': results.pvalues\n}).round(4)\n\nprint(summary_df)\n\n            Coefficient  Std. Error  z-value  p-value\nIntercept       -0.5089      0.1832  -2.7783   0.0055\nage              0.1486      0.0139  10.7162   0.0000\nage_sq          -0.0030      0.0003 -11.5132   0.0000\niscustomer       0.2076      0.0309   6.7192   0.0000\nNortheast        0.0292      0.0436   0.6686   0.5037\nNorthwest       -0.0176      0.0538  -0.3268   0.7438\nSouth            0.0566      0.0527   1.0740   0.2828\nSouthwest        0.0506      0.0472   1.0716   0.2839\n\n\n&lt;&gt;:17: SyntaxWarning:\n\ninvalid escape sequence '\\$'\n\n&lt;&gt;:17: SyntaxWarning:\n\ninvalid escape sequence '\\$'\n\nC:\\Users\\komal\\AppData\\Local\\Temp\\ipykernel_31016\\3702920505.py:17: SyntaxWarning:\n\ninvalid escape sequence '\\$'\n\n\n\nEstimating the same model with glm below :\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\nresults = poisson_model.fit()\nprint(results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:32:46   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n\n\nWe estimated a Poisson regression model to understand which listing characteristics predict the number of Airbnb reviews, used here as a proxy for bookings. Several variables emerged as significant predictors:\n\nInstant bookable status had the strongest effect: listings that allow instant booking received ~40% (exp(0.334)) more reviews, highlighting the importance of booking convenience.\nCleanliness score was another strong predictor: a 1-point increase in cleanliness rating corresponded to a ~12% (exp(0.113)) increase in reviews.\nBedrooms were positively associated with reviews — each additional bedroom was linked to an ~8% increase in expected bookings.\nIn contrast, shared rooms received ~22% fewer reviews than entire homes/apartments, and private rooms had a slight negative effect as well.\nBathrooms, review_scores_location, and review_scores_value showed negative associations with review count, suggesting that certain amenities or subjective perceptions may not directly translate to more bookings.\nThese results emphasize the importance of user experience factors (cleanliness, ease of booking) and listing type in attracting more guests.\nPseudo R squared = 0.5649, or ~56% This means the model explains about 56% of the variation in the number of reviews relative to a null model"
  },
  {
    "objectID": "projects/inclass/mle.html",
    "href": "projects/inclass/mle.html",
    "title": "Komal's Portfolio",
    "section": "",
    "text": "import pandas as pd\n\n\npurch_data = pd.read_csv('purchase.csv')\n\nUsing glm\n\nimport statsmodels.api as sm\n\n# Define the independent variable (with a constant for the intercept)\nX = sm.add_constant(purch_data['idx'])\n\n# Define the dependent variable\ny = purch_data['purchase']\n\n# Fit the GLM model with a binomial family (logistic regression)\nglm_model = sm.GLM(y, X, family=sm.families.Binomial()).fit()\nglm_model.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npurchase\nNo. Observations:\n2000\n\n\nModel:\nGLM\nDf Residuals:\n1998\n\n\nModel Family:\nBinomial\nDf Model:\n1\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-769.29\n\n\nDate:\nFri, 02 May 2025\nDeviance:\n1538.6\n\n\nTime:\n14:38:29\nPearson chi2:\n2.03e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.06354\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-3.2196\n0.160\n-20.175\n0.000\n-3.532\n-2.907\n\n\nidx\n0.0325\n0.003\n11.142\n0.000\n0.027\n0.038\n\n\n\n\n\n\n\n# Calculate the confidence intervals\nconfidence_intervals = glm_model.conf_int()\nprint(confidence_intervals)\n\n              0         1\nconst -3.532406 -2.906838\nidx    0.026805  0.038249\n\n\nusing manual method for MLE and calculating standard errors\nYou want to compute the standard errors of the estimated coefficients from your manual MLE (maximum likelihood estimation) of logistic regression.\nIn statistics, standard errors indicate how precise your parameter estimates are — they’re derived from the curvature (second derivatives) of the likelihood function, i.e., the Hessian matrix.\nWhen using scipy.optimize.minimize(), the result object can include the inverse Hessian (hess_inv), which approximates the covariance matrix of the estimated coefficients.\nThe Hessian is the matrix of second derivatives of the negative log-likelihood function\nThe diagonal of the inverse Hessian gives the variance estimates of the coefficients.\nTaking the square root gives the standard errors.\nThese are directly comparable to what glm_model.bse gives in the built-in statsmodels logistic regression.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\n# Prepare data\nX = sm.add_constant(purch_data[['idx']])\ny = purch_data['purchase']\n\nX_np = X.values\ny_np = y.values\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(beta, X, y):\n    z = np.dot(X, beta)\n    pi = 1 / (1 + np.exp(-z))\n    pi = np.clip(pi, 1e-8, 1 - 1e-8)\n    ll = np.sum(y * np.log(pi) + (1 - y) * np.log(1 - pi))\n    return -ll\n\n# Define the gradient (first derivative)\ndef grad_neg_log_likelihood(beta, X, y):\n    pi = 1 / (1 + np.exp(-np.dot(X, beta)))\n    return -np.dot(X.T, y - pi)\n\n# Define the Hessian (second derivative)\ndef hess_neg_log_likelihood(beta, X):\n    pi = 1 / (1 + np.exp(-np.dot(X, beta)))\n    W = np.diag(pi * (1 - pi))\n    return np.dot(X.T, np.dot(W, X))\n\n# Initial guess\ninitial_beta = np.zeros(X.shape[1])\n\n# Run minimization with gradient and Hessian\nresult = minimize(\n    neg_log_likelihood,\n    initial_beta,\n    args=(X_np, y_np),\n    method='Newton-CG',\n    jac=grad_neg_log_likelihood,\n    hess=lambda b, X, y: hess_neg_log_likelihood(b, X),\n    options={'xtol': 1e-8, 'disp': True}\n)\n\n# Estimated coefficients\nbeta_hat = result.x\nprint(\"Estimated coefficients:\", beta_hat)\n\n# Compute Hessian at optimum\nhessian = hess_neg_log_likelihood(beta_hat, X_np)\n\n# Invert Hessian to get variance-covariance matrix\ncov_matrix = np.linalg.inv(hessian)\nstandard_errors = np.sqrt(np.diag(cov_matrix))\nprint(\"Standard errors (manual):\", standard_errors)\n\n# 95% confidence intervals\nz = norm.ppf(0.975)\nlower = beta_hat - z * standard_errors\nupper = beta_hat + z * standard_errors\n\nfor i, (l, u) in enumerate(zip(lower, upper)):\n    print(f\"95% CI for beta_{i}: ({l:.4f}, {u:.4f})\")\n\nOptimization terminated successfully.\n         Current function value: 769.287143\n         Iterations: 23\n         Function evaluations: 37\n         Gradient evaluations: 37\n         Hessian evaluations: 23\nEstimated coefficients: [-3.21962193  0.03252663]\nStandard errors (manual): [0.15958658 0.00291937]\n95% CI for beta_0: (-3.5324, -2.9068)\n95% CI for beta_1: (0.0268, 0.0382)\n\n\nBootstrap method\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Setup\nX = sm.add_constant(purch_data[['idx']])\ny = purch_data['purchase']\nX_np = X.values\ny_np = y.values\n\n# Store slope coefficient (index 1 because index 0 is intercept)\nslope_boot = []\n\n# Bootstrap parameters\nn_boot = 1000\nnp.random.seed(42)\n\nfor _ in range(n_boot):\n    # Sample with replacement\n    idx_sample = np.random.choice(len(y_np), size=len(y_np), replace=True)\n    X_sample = X_np[idx_sample]\n    y_sample = y_np[idx_sample]\n\n    # Fit model on bootstrap sample\n    def neg_log_likelihood(beta, X, y):\n        pi = 1 / (1 + np.exp(-np.dot(X, beta)))\n        pi = np.clip(pi, 1e-8, 1 - 1e-8)\n        ll = np.sum(y * np.log(pi) + (1 - y) * np.log(1 - pi))\n        return -ll\n\n    res = minimize(\n        neg_log_likelihood,\n        x0=np.zeros(X_np.shape[1]),\n        args=(X_sample, y_sample),\n        method='BFGS'\n    )\n\n    # Store slope coefficient (beta[1])\n    if res.success:\n        slope_boot.append(res.x[1])\n\nslope_boot = np.array(slope_boot)\n\n\nslope_mean = np.mean(slope_boot)\nslope_std = np.std(slope_boot, ddof=1)  # sample std dev\nz = 1.96  # 95% CI\n\nci_std = (slope_mean - z * slope_std, slope_mean + z * slope_std)\nprint(f\"Bootstrap CI (Std Dev approach): {ci_std}\")\n\nBootstrap CI (Std Dev approach): (0.0271818769043601, 0.037844233597841)\n\n\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# Plot 1: Distribution of bootstrap slope coefficients\nplt.figure(figsize=(10, 6))\nplt.hist(slope_boot, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(slope_mean, color='red', linestyle='--', label=f'Mean: {slope_mean:.4f}')\nplt.title('Distribution of Bootstrap Slope Coefficients')\nplt.xlabel('Slope Coefficient')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn class 2\n\nmaxdiff = pd.read_excel('maxdiff.xlsx')\n\n\n# Count the occurrences of each Response value for each Item\nresponse_counts = maxdiff.groupby(['Item', 'Response']).size().unstack(fill_value=0)\n\n# Rename the columns for clarity\nresponse_counts.columns = ['Count of -1', 'Count of 0', 'Count of 1']\n# Sort the response_counts DataFrame by 'Count of 1' in descending order\nresponse_counts = response_counts.sort_values(by=['Count of -1'], ascending=False)\nprint(response_counts)\n\n      Count of -1  Count of 0  Count of 1\nItem                                     \n4              49          39          14\n1              41          42          22\n7              32          46          24\n3              31          54          14\n11             31          57          15\n8              26          19          58\n10             25          62          15\n6              20          37          43\n12             17          43          45\n9              16          77          10\n13             16          78           9\n2              15          63          23\n5              14          49          41"
  },
  {
    "objectID": "projects/MLE/hw2_questions.html#likelihood-function-for-poisson-distribution",
    "href": "projects/MLE/hw2_questions.html#likelihood-function-for-poisson-distribution",
    "title": "Poisson Regression Examples",
    "section": "Likelihood Function for Poisson Distribution",
    "text": "Likelihood Function for Poisson Distribution\nLet \\(Y_1, Y_2, \\dots, Y_n \\sim \\text{Poisson}(\\lambda)\\) be independent observations.\nThe probability mass function for a single observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe likelihood function for the entire sample is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\frac{1}{\\prod_{i=1}^{n} Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log Y_i!\n\\] todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html",
    "href": "projects/Conjoint/hw3_questions.html",
    "title": "MLE and COnjoint Analysis",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/Conjoint/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "MLE and COnjoint Analysis",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/Conjoint/hw3_questions.html#simulate-conjoint-data",
    "title": "MLE and COnjoint Analysis",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = list(range(8, 33, 4))\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brands for a in ads for p in prices],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Simulate one respondent's data\ndef sim_one(resp_id):\n    tasks = []\n\n    for t in range(1, n_tasks + 1):\n        # Sample 3 alternatives randomly\n        sampled = profiles.sample(n=n_alts, replace=False).copy()\n        sampled[\"resp\"] = resp_id\n        sampled[\"task\"] = t\n\n        # Compute deterministic utility\n        sampled[\"v\"] = sampled[\"brand\"].map(b_util) + \\\n                       sampled[\"ad\"].map(a_util) + \\\n                       sampled[\"price\"].apply(p_util)\n        \n        # Add Gumbel noise (Type I extreme value)\n        sampled[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        # Determine choice\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        tasks.append(sampled[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]])\n\n    return pd.concat(tasks)\n\n# Simulate for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\nconjoint_data\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2995\n100\n9\nP\nYes\n12\n1\n\n\n2996\n100\n9\nP\nNo\n24\n0\n\n\n2997\n100\n10\nH\nNo\n20\n0\n\n\n2998\n100\n10\nH\nYes\n8\n0\n\n\n2999\n100\n10\nN\nNo\n16\n1\n\n\n\n\n3000 rows × 6 columns"
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/Conjoint/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "MLE and COnjoint Analysis",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\ndf = pd.get_dummies(conjoint_data, columns=['brand', 'ad'], drop_first=True)\nbool_cols = df.select_dtypes(include='bool').columns\ndf[bool_cols] = df[bool_cols].astype(int)\ndf.head()\n\n\n\n\n\n\n\n\nresp\ntask\nprice\nchoice\nbrand_N\nbrand_P\nad_Yes\n\n\n\n\n0\n1\n1\n32\n0\n0\n1\n0\n\n\n1\n1\n1\n28\n0\n1\n0\n0\n\n\n2\n1\n1\n24\n1\n1\n0\n0\n\n\n3\n1\n2\n28\n0\n0\n0\n0\n\n\n4\n1\n2\n8\n1\n0\n0\n0"
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/Conjoint/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "MLE and COnjoint Analysis",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nNext we define the maximum likelihood function to find the \\(\\beta\\) values for each parameter.\n\nX_cols = ['price', 'brand_N', 'brand_P', 'ad_Yes']\nX = df[X_cols].values\ny = df['choice'].values\n\nn_alts = 3\nn_sets = int(len(df) / n_alts)\n\ndef negative_log_likelihood(beta):\n    ll = 0\n    for i in range(n_sets):\n        start = i * n_alts\n        end = start + n_alts\n        X_i = X[start:end]\n        y_i = y[start:end]\n        utilities = X_i @ beta\n        utilities -= np.max(utilities)\n        exp_utilities = np.exp(utilities)\n        probabilities = exp_utilities / np.sum(exp_utilities)\n        ll += np.sum(y_i * np.log(probabilities + 1e-12))\n    return -ll\n\nFurther, we use the scipy.optimize() function to find the MLEs for the 4 parameters as well as their standard errors (from the Hessian). We also estimate a 95% confidence interval for each parameter estimate.\n\nprint(\"MLE Estimation of beta\")\nfrom scipy.optimize import minimize\nimport scipy.stats as stats\n# Initial guess for parameters (zeros)\ninitial_beta = np.zeros(X.shape[1])\n\n# Estimate MLE using BFGS method\nresult = minimize(negative_log_likelihood, initial_beta, method='BFGS')\n\n# Estimated parameters\nbeta_hat = result.x\n\n# Inverse of Hessian gives variance-covariance matrix\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# 95% Confidence Intervals\nz = stats.norm.ppf(0.975)  # ≈ 1.96\nconf_intervals = [(b - z*se, b + z*se) for b, se in zip(beta_hat, standard_errors)]\nmle_results={}\n# Display results\nfor name, b, se, ci in zip(X_cols, beta_hat, standard_errors, conf_intervals):\n    print(f\"{name}: beta = {b:.4f}, SE = {se:.4f}, 95% CI = ({ci[0]:.4f}, {ci[1]:.4f})\")\n    mle_results[name] = {\n        \"beta\": b,\n        \"se\": se,\n        \"ci\": (ci[0], ci[1])\n    }\n\nMLE Estimation of beta\nprice: beta = -0.0964, SE = 0.0061, 95% CI = (-0.1084, -0.0844)\nbrand_N: beta = 1.0569, SE = 0.0120, 95% CI = (1.0333, 1.0804)\nbrand_P: beta = 0.4733, SE = 0.0709, 95% CI = (0.3344, 0.6122)\nad_Yes: beta = -0.7724, SE = 0.0313, 95% CI = (-0.8337, -0.7111)"
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/Conjoint/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "MLE and COnjoint Analysis",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nIn this section we are estimating the posterior distribution of 4 parameters in a MNL model using Bayesian inference. In other words we try to estimate “Based on the observed choices (data), and what I believed about the parameters before (priors), what should I now believe (posterior)”\nThe Baye’s rule is given by -\n\\[\nP(\\beta \\mid \\text{data}) \\propto {P(\\text{data} \\mid \\beta) \\cdot P(\\beta)}\n\\]\nwhere \\(P(\\beta)\\) = Prior \\(P(\\text{data} \\mid \\beta)\\) = Likelihood \\(P(\\beta \\mid \\text{data})\\) = Posterior\n\nn_alts = 3\nn_sets = int(len(df) / n_alts)\nn_params = X.shape[1]\n\ndef log_prior(beta):\n    logp_price = -0.5 * (beta[0] / 1)**2\n    logp_binaries = -0.5 * np.sum((beta[1:] / 5)**2)\n    return logp_price + logp_binaries\n\ndef log_posterior(beta):\n    return log_prior(beta) - negative_log_likelihood(beta) \n\n#Metropolis-Hastings MCMC Sampler\nn_samples = 11000\nburn_in = 1000\nsamples = np.zeros((n_samples, n_params))\n\n# Initial guess\ncurrent_beta = np.zeros(n_params)\ncurrent_log_post = log_posterior(current_beta)\n\n# Proposal SDs: binary vars = 0.05, price = 0.005\nproposal_sds = np.array([0.005, 0.05, 0.05, 0.05])\n\nfor t in range(1, n_samples):\n    proposal = current_beta + np.random.normal(0, proposal_sds)\n    proposal_log_post = log_posterior(proposal)\n    accept_prob = min(1, np.exp(proposal_log_post - current_log_post))\n\n    if np.random.rand() &lt; accept_prob:\n        current_beta = proposal\n        current_log_post = proposal_log_post\n\n    samples[t] = current_beta\n\n# Drop burn-in\nposterior_samples = samples[burn_in:]\nbayes_results={}\n# Posterior summaries\nfor i, name in enumerate(X_cols):\n    param_samples = posterior_samples[:, i]\n    mean = np.mean(param_samples)\n    std = np.std(param_samples)\n    ci_low, ci_high = np.percentile(param_samples, [2.5, 97.5])\n    print(f\"{name}: Beta = {mean:.4f}, SD = {std:.4f}, 95% CI = ({ci_low:.4f}, {ci_high:.4f})\")\n    bayes_results[name] = {\n        \"beta\": mean,\n        \"std\": std,\n        \"ci\": (ci_low, ci_high)\n    }\n\nprice: Beta = -0.0968, SD = 0.0062, 95% CI = (-0.1092, -0.0849)\nbrand_N: Beta = 1.0599, SD = 0.1138, 95% CI = (0.8444, 1.3003)\nbrand_P: Beta = 0.4845, SD = 0.1136, 95% CI = (0.2721, 0.7098)\nad_Yes: Beta = -0.7801, SD = 0.0918, 95% CI = (-0.9606, -0.5981)\n\n\nBelow is the trace plot of the algorithm, as well as the histogram of the posterior distribution for the price estimate.\n\nprint(posterior_samples)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming price is at index 3\nprice_samples = posterior_samples[:, 0]\n\n# Plot\nplt.figure(figsize=(12, 4))\n\n# Trace Plot\n# Trace Plot\nplt.plot(price_samples, color='blue', linewidth=0.7)\nplt.title(\"Trace Plot: Price Coefficient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"β (price)\")\nplt.show()\n\n# Posterior Histogram\nplt.hist(price_samples, bins=40, color='skyblue', edgecolor='black', density=True)\nplt.title(\"Posterior Distribution: Price Coefficient\")\nplt.xlabel(\"β (price)\")\nplt.ylabel(\"Density\")\nplt.show()\n\n[[-0.10650537  1.27974524  0.71792105 -0.81615042]\n [-0.10292188  1.22902364  0.71818284 -0.80961323]\n [-0.10292188  1.22902364  0.71818284 -0.80961323]\n ...\n [-0.09516726  1.19575878  0.68254034 -0.69301451]\n [-0.09442549  1.11632281  0.6523968  -0.69341429]\n [-0.09442549  1.11632281  0.6523968  -0.69341429]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trace plot for the price coefficient shows stable oscillations around a central value, indicating that the Markov Chain Monte Carlo (MCMC) sampler has converged and is mixing well. There is no upward or downward drift, suggesting the chain is sampling effectively from the posterior distribution.\nThe corresponding posterior distribution is approximately normal, centered around -0.10. This confirms a strong negative effect of price on product choice — as price increases, the probability of selection decreases. The tight, symmetric shape of the distribution reflects high certainty in this estimate.\nThe following table summarizes the estimates from MLE and Bayesian methods.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate (SE)\n95% CI (MLE)\nBayes Mean (SD)\n95% Credible Interval\n\n\n\n\nprice\n-0.0964 (0.0061)\n(-0.1084, -0.0844)\n-0.0968 (0.0062)\n(-0.1092, -0.0849)\n\n\nbrand_N\n1.0569 (0.0120)\n(1.0333, 1.0804)\n1.0599 (0.1138)\n(0.8444, 1.3003)\n\n\nbrand_P\n0.4733 (0.0709)\n(0.3344, 0.6122)\n0.4845 (0.1136)\n(0.2721, 0.7098)\n\n\nad_Yes\n-0.7724 (0.0313)\n(-0.8337, -0.7111)\n-0.7801 (0.0918)\n(-0.9606, -0.5981)"
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#discussion",
    "href": "projects/Conjoint/hw3_questions.html#discussion",
    "title": "MLE and COnjoint Analysis",
    "section": "6. Discussion",
    "text": "6. Discussion\nInterpretation of Parameter Estimates If the data were not simulated, the parameter estimates would reflect real-world consumer preferences inferred from observed choices. In that case:\nA finding like $  &gt;  $ means that, on average, consumers derive more utility from choosing Netflix compared to Amazon Prime, holding all other attributes constant.\nThis could indicate that consumers perceive higher value or satisfaction from Netflix’s offering (e.g., content library, user experience).\nIt also implies that, in the utility function a higher coefficient on Netflix leads to a higher probability of being chosen.\nRegarding price:\nA negative \\(\\beta_\\text{price}\\) is expected and intuitive. It means that, all else equal, an increase in price decreases utility, which lowers the likelihood of the product being chosen.\nThis reflects basic economic theory: consumers prefer lower-cost options when utility from other features is equal.\n\nMulti-Level (Hierarchical) Model\nIn real-world conjoint analysis, consumer preferences are rarely homogeneous. The basic multinomial logit (MNL) model assumes that every individual shares the same set of preference parameters (\\(\\beta\\)), which is a strong and often unrealistic assumption.\nTo better reflect real-world heterogeneity in preferences, we use a multi-level model. Here’s how this model can be both simulated and estimated:\n1. Simulating Individual-Level Preferences\nInstead of one global \\(\\beta\\), we assume each respondent \\(i\\) has their own parameter vector \\({\\beta}_i\\), drawn from a common population distribution:\n\\[\n{\\beta}_i \\sim \\mathcal{N}({\\mu}, {\\Sigma})\n\\]\n\n\\({\\mu}\\): population mean of preferences\n\n\\({\\Sigma}\\): covariance matrix capturing variability and correlations between parameters\n\nThis framework captures variation in individual tastes.\n\n2. Simulating Choice Data\nUsing each individual’s \\({\\beta}_i\\), simulate their choices for each task using the softmax choice probability:\n\\[\nP_{ij} = \\frac{\\exp(\\mathbf{X}_{ij}^\\top {\\beta}_i)}{\\sum_k \\exp(\\mathbf{X}_{ik}^\\top {\\beta}_i)}\n\\]\nWhere: - \\(j\\) indexes alternatives - \\(\\mathbf{X}_{ij}\\) are the attributes of alternative \\(j\\) - \\(P_{ij}\\) is the probability that individual \\(i\\) chooses alternative \\(j\\)\n\n3. Estimating the Model: Hierarchical Bayes\nTo estimate a hierarchical model, use Hierarchical Bayesian (HB) methods such as:\n\nGibbs sampling\n\nHamiltonian Monte Carlo\nMCMC within Gibbs (as in traditional HB packages)\n\nThese methods estimate:\n\nIndividual-level coefficients \\(\\beta_i\\)\nPopulation-level parameters \\(\\mu\\), \\(\\Sigma\\)\n\nThis approach gives: - Personalized preference estimates - More realistic modeling of population behavior - Better predictive performance"
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#comparison-bayesian-vs.-mle-estimates",
    "href": "projects/Conjoint/hw3_questions.html#comparison-bayesian-vs.-mle-estimates",
    "title": "Multinomial Logit Model",
    "section": "📊 Comparison: Bayesian vs. MLE Estimates",
    "text": "📊 Comparison: Bayesian vs. MLE Estimates\nWe compare the posterior means and uncertainty intervals from the Bayesian estimation (using Metropolis-Hastings MCMC) to the point estimates and standard errors from the Maximum Likelihood Estimation (MLE).\n\n1. Price\n\nBayesian:\nMean = -0.0965, SD = 0.0061\n95% Credible Interval: \\((-0.1085,\\ -0.0849)\\)\nMLE:\n\\(\\beta = -0.0964\\), SE = 0.0061\n95% Confidence Interval: \\((-0.1084,\\ -0.0844)\\)\n\n✅ Strong agreement. Both methods indicate a significant negative effect of price on choice probability.\n\n\n\n2. Brand N\n\nBayesian:\nMean = 1.0585, SD = 0.1082\n95% Credible Interval: \\((0.8553,\\ 1.2727)\\)\nMLE:\n\\(\\beta = 1.0569\\), SE = 0.0120\n95% Confidence Interval: \\((1.0333,\\ 1.0804)\\)\n\n🟡 Similar point estimates, but the Bayesian interval is wider, reflecting more uncertainty.\n\n\n\n3. Brand P\n\nBayesian:\nMean = 0.4706, SD = 0.1016\n95% Credible Interval: \\((0.2722,\\ 0.6652)\\)\nMLE:\n\\(\\beta = 0.4733\\), SE = 0.0709\n95% Confidence Interval: \\((0.3344,\\ 0.6122)\\)\n\n🟡 Moderate alignment. Bayesian interval again shows greater uncertainty.\n\n\n\n4. Ad Exposure (ad_Yes)\n\nBayesian:\nMean = -0.7715, SD = 0.0895\n95% Credible Interval: \\((-0.9539,\\ -0.6019)\\)\nMLE:\n\\(\\beta = -0.7724\\), SE = 0.0313\n95% Confidence Interval: \\((-0.8337,\\ -0.7111)\\)\n\n✅ Excellent agreement on both the direction and magnitude of the effect. Bayesian posterior accounts for more variability.\n\n\n\n📌 Summary Table\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate (SE)\nBayesian Mean (SD)\nInterpretation\n\n\n\n\nPrice\n-0.0964 (0.0061)\n-0.0965 (0.0061)\nStrong negative effect, consistent\n\n\nBrand N\n1.0569 (0.0120)\n1.0585 (0.1082)\nStrong positive, more uncertain in Bayes\n\n\nBrand P\n0.4733 (0.0709)\n0.4706 (0.1016)\nModerate positive effect\n\n\nAd (Yes)\n-0.7724 (0.0313)\n-0.7715 (0.0895)\nStrong negative effect\n\n\n\n\n\n\n🧠 Conclusion\nThe MLE and Bayesian estimates agree closely on the direction and relative importance of all parameters. The Bayesian model, however, provides a more complete picture of uncertainty through wider credible intervals, especially for the brand and ad effects."
  },
  {
    "objectID": "projects/Conjoint/hw3_questions.html#multi-level-hierarchical-model",
    "href": "projects/Conjoint/hw3_questions.html#multi-level-hierarchical-model",
    "title": "Multinomial Logit Model",
    "section": "Multi-Level (Hierarchical) Model",
    "text": "Multi-Level (Hierarchical) Model\nIn real-world conjoint analysis, consumer preferences are rarely homogeneous. The basic multinomial logit (MNL) model assumes that every individual shares the same set of preference parameters (\\(\\beta\\)), which is a strong and often unrealistic assumption.\nTo better reflect real-world heterogeneity in preferences, we use a multi-level model. Here’s how this model can be both simulated and estimated:\n\n1. Simulating Individual-Level Preferences\nInstead of one global \\(\\beta\\), we assume each respondent \\(i\\) has their own parameter vector \\({\\beta}_i\\), drawn from a common population distribution:\n\\[\n{\\beta}_i \\sim \\mathcal{N}({\\mu}, {\\Sigma})\n\\]\n\n\\({\\mu}\\): population mean of preferences\n\n\\({\\Sigma}\\): covariance matrix capturing variability and correlations between parameters\n\nThis framework captures variation in individual tastes.\n\n\n\n2. Simulating Choice Data\nUsing each individual’s \\({\\beta}_i\\), simulate their choices for each task using the softmax choice probability:\n\\[\nP_{ij} = \\frac{\\exp(\\mathbf{X}_{ij}^\\top {\\beta}_i)}{\\sum_k \\exp(\\mathbf{X}_{ik}^\\top {\\beta}_i)}\n\\]\nWhere: - \\(j\\) indexes alternatives - \\(\\mathbf{X}_{ij}\\) are the attributes of alternative \\(j\\) - \\(P_{ij}\\) is the probability that individual \\(i\\) chooses alternative \\(j\\)\n\n\n\n3. Estimating the Model: Hierarchical Bayes\nTo estimate a hierarchical model, use Hierarchical Bayesian (HB) methods such as:\n\nGibbs sampling\n\nHamiltonian Monte Carlo\nMCMC within Gibbs (as in traditional HB packages)\n\nThese methods estimate:\n\nIndividual-level coefficients \\(\\beta_i\\)\nPopulation-level parameters \\(\\mu\\), \\(\\Sigma\\)\n\nThis approach gives: - Personalized preference estimates - More realistic modeling of population behavior - Better predictive performance"
  }
]