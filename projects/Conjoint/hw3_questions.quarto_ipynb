{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"MLE and COnjoint Analysis\"\n",
        "author: \"Komal Nagaraj\"\n",
        "date: today\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}"
      ],
      "id": "39a8946a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define attributes\n",
        "brands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\n",
        "ads = [\"Yes\", \"No\"]\n",
        "prices = list(range(8, 33, 4))\n",
        "\n",
        "# Generate all possible profiles\n",
        "profiles = pd.DataFrame(\n",
        "    [(b, a, p) for b in brands for a in ads for p in prices],\n",
        "    columns=[\"brand\", \"ad\", \"price\"]\n",
        ")\n",
        "m = len(profiles)\n",
        "\n",
        "# Assign part-worth utilities\n",
        "b_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\n",
        "a_util = {\"Yes\": -0.8, \"No\": 0.0}\n",
        "p_util = lambda p: -0.1 * p\n",
        "\n",
        "# Parameters\n",
        "n_peeps = 100\n",
        "n_tasks = 10\n",
        "n_alts = 3\n",
        "\n",
        "# Simulate one respondent's data\n",
        "def sim_one(resp_id):\n",
        "    tasks = []\n",
        "\n",
        "    for t in range(1, n_tasks + 1):\n",
        "        # Sample 3 alternatives randomly\n",
        "        sampled = profiles.sample(n=n_alts, replace=False).copy()\n",
        "        sampled[\"resp\"] = resp_id\n",
        "        sampled[\"task\"] = t\n",
        "\n",
        "        # Compute deterministic utility\n",
        "        sampled[\"v\"] = sampled[\"brand\"].map(b_util) + \\\n",
        "                       sampled[\"ad\"].map(a_util) + \\\n",
        "                       sampled[\"price\"].apply(p_util)\n",
        "        \n",
        "        # Add Gumbel noise (Type I extreme value)\n",
        "        sampled[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n",
        "        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n",
        "\n",
        "        # Determine choice\n",
        "        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n",
        "\n",
        "        tasks.append(sampled[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]])\n",
        "\n",
        "    return pd.concat(tasks)\n",
        "\n",
        "# Simulate for all respondents\n",
        "conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n",
        "conjoint_data"
      ],
      "id": "e80bbcad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n"
      ],
      "id": "f0fd94ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.get_dummies(conjoint_data, columns=['brand', 'ad'], drop_first=True)\n",
        "bool_cols = df.select_dtypes(include='bool').columns\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "df.head()"
      ],
      "id": "aa08068d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "Next we define the maximum likelihood function to find the  $\\beta$ values for each parameter.\n"
      ],
      "id": "a13e1566"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_cols = ['price', 'brand_N', 'brand_P', 'ad_Yes']\n",
        "X = df[X_cols].values\n",
        "y = df['choice'].values\n",
        "\n",
        "n_alts = 3\n",
        "n_sets = int(len(df) / n_alts)\n",
        "\n",
        "def negative_log_likelihood(beta):\n",
        "    ll = 0\n",
        "    for i in range(n_sets):\n",
        "        start = i * n_alts\n",
        "        end = start + n_alts\n",
        "        X_i = X[start:end]\n",
        "        y_i = y[start:end]\n",
        "        utilities = X_i @ beta\n",
        "        utilities -= np.max(utilities)\n",
        "        exp_utilities = np.exp(utilities)\n",
        "        probabilities = exp_utilities / np.sum(exp_utilities)\n",
        "        ll += np.sum(y_i * np.log(probabilities + 1e-12))\n",
        "    return -ll"
      ],
      "id": "7a83c719",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further, we use the scipy.optimize() function to find the MLEs for the 4 parameters as well as  their standard errors (from the Hessian). We also estimate a 95% confidence interval for each parameter estimate.\n"
      ],
      "id": "1ded1ddf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"MLE Estimation of beta\")\n",
        "from scipy.optimize import minimize\n",
        "import scipy.stats as stats\n",
        "# Initial guess for parameters (zeros)\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "\n",
        "# Estimate MLE using BFGS method\n",
        "result = minimize(negative_log_likelihood, initial_beta, method='BFGS')\n",
        "\n",
        "# Estimated parameters\n",
        "beta_hat = result.x\n",
        "\n",
        "# Inverse of Hessian gives variance-covariance matrix\n",
        "hessian_inv = result.hess_inv\n",
        "standard_errors = np.sqrt(np.diag(hessian_inv))\n",
        "\n",
        "# 95% Confidence Intervals\n",
        "z = stats.norm.ppf(0.975)  # ≈ 1.96\n",
        "conf_intervals = [(b - z*se, b + z*se) for b, se in zip(beta_hat, standard_errors)]\n",
        "mle_results={}\n",
        "# Display results\n",
        "for name, b, se, ci in zip(X_cols, beta_hat, standard_errors, conf_intervals):\n",
        "    print(f\"{name}: beta = {b:.4f}, SE = {se:.4f}, 95% CI = ({ci[0]:.4f}, {ci[1]:.4f})\")\n",
        "    mle_results[name] = {\n",
        "        \"beta\": b,\n",
        "        \"se\": se,\n",
        "        \"ci\": (ci[0], ci[1])\n",
        "    }"
      ],
      "id": "cc307410",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estimation via Bayesian Methods\n",
        "In this section we are estimating the posterior distribution of 4 parameters in a MNL model using Bayesian inference. In other words we try to estimate \"Based on the observed choices (data), and what I believed about the parameters before (priors), what should I now believe (posterior)\"\n",
        "\n",
        "The Baye's rule is given by - \n",
        "\n",
        "$$\n",
        "P(\\beta \\mid \\text{data}) \\propto {P(\\text{data} \\mid \\beta) \\cdot P(\\beta)}\n",
        "$$\n",
        "\n",
        "where $P(\\beta)$ = Prior\n",
        "$P(\\text{data} \\mid \\beta)$ = Likelihood\n",
        "$P(\\beta \\mid \\text{data})$ = Posterior\n"
      ],
      "id": "cc4f6bc5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_alts = 3\n",
        "n_sets = int(len(df) / n_alts)\n",
        "n_params = X.shape[1]\n",
        "\n",
        "def log_prior(beta):\n",
        "    logp_price = -0.5 * (beta[0] / 1)**2\n",
        "    logp_binaries = -0.5 * np.sum((beta[1:] / 5)**2)\n",
        "    return logp_price + logp_binaries\n",
        "\n",
        "def log_posterior(beta):\n",
        "    return log_prior(beta) - negative_log_likelihood(beta) \n",
        "\n",
        "#Metropolis-Hastings MCMC Sampler\n",
        "n_samples = 11000\n",
        "burn_in = 1000\n",
        "samples = np.zeros((n_samples, n_params))\n",
        "\n",
        "# Initial guess\n",
        "current_beta = np.zeros(n_params)\n",
        "current_log_post = log_posterior(current_beta)\n",
        "\n",
        "# Proposal SDs: binary vars = 0.05, price = 0.005\n",
        "proposal_sds = np.array([0.005, 0.05, 0.05, 0.05])\n",
        "\n",
        "for t in range(1, n_samples):\n",
        "    proposal = current_beta + np.random.normal(0, proposal_sds)\n",
        "    proposal_log_post = log_posterior(proposal)\n",
        "    accept_prob = min(1, np.exp(proposal_log_post - current_log_post))\n",
        "\n",
        "    if np.random.rand() < accept_prob:\n",
        "        current_beta = proposal\n",
        "        current_log_post = proposal_log_post\n",
        "\n",
        "    samples[t] = current_beta\n",
        "\n",
        "# Drop burn-in\n",
        "posterior_samples = samples[burn_in:]\n",
        "bayes_results={}\n",
        "# Posterior summaries\n",
        "for i, name in enumerate(X_cols):\n",
        "    param_samples = posterior_samples[:, i]\n",
        "    mean = np.mean(param_samples)\n",
        "    std = np.std(param_samples)\n",
        "    ci_low, ci_high = np.percentile(param_samples, [2.5, 97.5])\n",
        "    print(f\"{name}: Beta = {mean:.4f}, SD = {std:.4f}, 95% CI = ({ci_low:.4f}, {ci_high:.4f})\")\n",
        "    bayes_results[name] = {\n",
        "        \"beta\": mean,\n",
        "        \"std\": std,\n",
        "        \"ci\": (ci_low, ci_high)\n",
        "    }"
      ],
      "id": "6e986ae7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the trace plot of the algorithm, as well as the histogram of the posterior distribution for the price estimate.\n"
      ],
      "id": "8ff0fdc3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(posterior_samples)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming price is at index 3\n",
        "price_samples = posterior_samples[:, 0]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Trace Plot\n",
        "# Trace Plot\n",
        "plt.plot(price_samples, color='blue', linewidth=0.7)\n",
        "plt.title(\"Trace Plot: Price Coefficient\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"β (price)\")\n",
        "plt.show()\n",
        "\n",
        "# Posterior Histogram\n",
        "plt.hist(price_samples, bins=40, color='skyblue', edgecolor='black', density=True)\n",
        "plt.title(\"Posterior Distribution: Price Coefficient\")\n",
        "plt.xlabel(\"β (price)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()"
      ],
      "id": "e0c1f43b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trace plot for the price coefficient shows stable oscillations around a central value, indicating that the Markov Chain Monte Carlo (MCMC) sampler has converged and is mixing well. There is no upward or downward drift, suggesting the chain is sampling effectively from the posterior distribution.\n",
        "\n",
        "The corresponding posterior distribution is approximately normal, centered around -0.10. This confirms a strong negative effect of price on product choice — as price increases, the probability of selection decreases. The tight, symmetric shape of the distribution reflects high certainty in this estimate.\n",
        "\n",
        "The following table summarizes the estimates from MLE and Bayesian methods.\n"
      ],
      "id": "1ad10dba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Build table header\n",
        "table_md = \"| Parameter | MLE Estimate (SE) | 95% CI (MLE) | Bayes Mean (SD) | 95% Credible Interval |\\n\"\n",
        "table_md += \"|-----------|-------------------|--------------|------------------|------------------------|\\n\"\n",
        "\n",
        "# Fill in rows dynamically\n",
        "for param in ['price', 'brand_N', 'brand_P', 'ad_Yes']:\n",
        "    mle = mle_results[param]\n",
        "    bayes = bayes_results[param]\n",
        "    \n",
        "    row = (\n",
        "        f\"| **{param}** \"\n",
        "        f\"| {mle['beta']:.4f} ({mle['se']:.4f}) \"\n",
        "        f\"| ({mle['ci'][0]:.4f}, {mle['ci'][1]:.4f}) \"\n",
        "        f\"| {bayes['beta']:.4f} ({bayes['std']:.4f}) \"\n",
        "        f\"| ({bayes['ci'][0]:.4f}, {bayes['ci'][1]:.4f}) |\\n\"\n",
        "    )\n",
        "    table_md += row\n",
        "\n",
        "# Display the Markdown-formatted table\n",
        "Markdown(table_md)"
      ],
      "id": "a095856b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Discussion\n",
        "\n",
        "Interpretation of Parameter Estimates\n",
        "If the data were not simulated, the parameter estimates would reflect real-world consumer preferences inferred from observed choices. In that case:\n",
        "\n",
        "A finding like $ \\beta_\\text{Netflix} > \\beta_\\text{Prime} $ means that, on average, consumers derive more utility from choosing Netflix compared to Amazon Prime, holding all other attributes constant.\n",
        "\n",
        "This could indicate that consumers perceive higher value or satisfaction from Netflix’s offering (e.g., content library, user experience).\n",
        "\n",
        "It also implies that, in the utility function a higher coefficient on Netflix leads to a higher probability of being chosen.\n",
        "\n",
        "Regarding price:\n",
        "\n",
        "A negative $\\beta_\\text{price}$ is expected and intuitive. It means that, all else equal, an increase in price decreases utility, which lowers the likelihood of the product being chosen.\n",
        "\n",
        "This reflects basic economic theory: consumers prefer lower-cost options when utility from other features is equal.\n",
        "\n",
        "#### Multi-Level (Hierarchical) Model\n",
        "\n",
        "In real-world conjoint analysis, consumer preferences are rarely homogeneous. The basic multinomial logit (MNL) model assumes that every individual shares the same set of preference parameters ($\\beta$), which is a strong and often unrealistic assumption.\n",
        "\n",
        "To better reflect real-world heterogeneity in preferences, we use a **multi-level** model. Here's how this model can be both simulated and estimated:\n",
        "\n",
        "**1. Simulating Individual-Level Preferences**\n",
        "\n",
        "Instead of one global $\\beta$, we assume each respondent $i$ has their own parameter vector ${\\beta}_i$, drawn from a common population distribution:\n",
        "\n",
        "$$\n",
        "{\\beta}_i \\sim \\mathcal{N}({\\mu}, {\\Sigma})\n",
        "$$\n",
        "\n",
        "- ${\\mu}$: population mean of preferences  \n",
        "- ${\\Sigma}$: covariance matrix capturing variability and correlations between parameters  \n",
        "\n",
        "This framework captures variation in individual tastes.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Simulating Choice Data**\n",
        "\n",
        "Using each individual's ${\\beta}_i$, simulate their choices for each task using the softmax choice probability:\n",
        "\n",
        "$$\n",
        "P_{ij} = \\frac{\\exp(\\mathbf{X}_{ij}^\\top {\\beta}_i)}{\\sum_k \\exp(\\mathbf{X}_{ik}^\\top {\\beta}_i)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $j$ indexes alternatives\n",
        "- $\\mathbf{X}_{ij}$ are the attributes of alternative $j$\n",
        "- $P_{ij}$ is the probability that individual $i$ chooses alternative $j$\n",
        "\n",
        "---\n",
        "\n",
        "**3. Estimating the Model: Hierarchical Bayes**\n",
        "\n",
        "To estimate a hierarchical model, use Hierarchical Bayesian (HB) methods such as:\n",
        "\n",
        "- Gibbs sampling  \n",
        "- Hamiltonian Monte Carlo \n",
        "- MCMC within Gibbs (as in traditional HB packages)\n",
        "\n",
        "These methods estimate:\n",
        "\n",
        "- Individual-level coefficients $\\beta_i$\n",
        "- Population-level parameters $\\mu$, $\\Sigma$\n",
        "\n",
        "This approach gives:\n",
        "- Personalized preference estimates\n",
        "- More realistic modeling of population behavior\n",
        "- Better predictive performance\n",
        "\n",
        "---\n"
      ],
      "id": "aa7361e3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\ProgramData\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}