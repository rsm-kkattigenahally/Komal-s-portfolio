mdat1 <- dfidx::dfidx(sub1, choice = "choice", shape = "long", chid.var = "customer_id")
out4 <- mlogit(choice ~ factor(phone_id) + price | 0, data = mdat1)
summary(out4)
brand_hit_rate <- function(data, model) {
# here we use the model to predict which phone maximizes each customer's utility
preds <- apply(predict(model, newdata = data), 1, which.max)
# here we construct a vector of customer choices for comparisons to predictions
actuals <- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)
# here we compare the model's predictions to the data
mean(ceiling(preds / 2) == ceiling(actuals / 2))
}
# now we'll do the same steps but at the phone level
product_hit_rate <- function(data, model) {
preds <- apply(predict(model, newdata = data), 1, which.max)
actuals <- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)
mean(preds == actuals)
}
ll_ratio <- function(data, model) {
N <- nrow(model$probabilities)
J <- ncol(model$probabilities)
ll0 <- N * log(1 / J)   # this is our null model for comparison
ll1 <- as.numeric(model$logLik)   # this is lnL(beta) from slides
1 - ll1 / ll0
}
brand_hit_rate(mdat1, out4)
product_hit_rate(mdat1, out4)
ll_ratio(mdat1, out4)
# Let's look at how histograms of choice probabilities for A1 change when phone
# discounts change.
myplot(mdat1, out4, "A1")
# X axis is predicted choice of A1, given a particular phone on discount
# Y axis shows how many customers were treated with each of 6 possible discounts
# Notice: Empirical probability of discount is not equal for all phones
# Run this command to verify:
cust_dat |> filter(years_ago==1) |> select(discount) |> table()
# Main point: Homogeneous demand model predicts a single response of choice
#     probability to price discount, because it assumes all customers have the
#     the same price responsiveness alpha. This will change shortly.
# Let's enrich this model to add heterogeneity
# We'll start out by adding individual heterogeneity by customer attributes
#     via a price*minutes interaction
# This is like assuming that willingness to pay for a smartphone may
#     increase with how often you use it
# What does the distribution of total_minutes look like?
ggplot(sub1, aes(x=total_minutes)) +
geom_histogram(binwidth = 100) +
xlab("Total Minutes") +
ylab("Count of Customers") +
ggtitle("Histogram of Total Minutes") +
theme_bw()
# We will hypothesize that customers that use their phones more are people who are
# less price sensitive when buying a phone. To test this, we include the interaction
# of "price" and "total minutes" in the model.
# We expect to see a positive estimated coefficient for this interaction
# We also expect the main effect of price to remain negative
out5 <- mlogit(choice ~ factor(phone_id) + price + price:total_minutes | 0, data = mdat1)
print(summary(out5))
brand_hit_rate(mdat1, out5)
product_hit_rate(mdat1, out5)
ll_ratio(mdat1, out5)
# Let's look at how our heterogeneous demand model draws histograms of choice
#     probabilities for A1 sales change when phone discounts change
myplot(mdat1, out5, "A1")
# We see that different customers have different probabilities of purchasing
# the same phone, even for customers facing the same discount. This is because
# we have included the interaction term in the model.
# What does it mean to include the interaction?  It means the marginal disutility
# of price is a function of the customer's needs, as reflected in past usage behavior.
# We can re-write part of model from:
#
# ... (alpha_int * price) + (alpha_minutes * price * totalminutes) ...
#
# to this
#
# ... (alpha_int + alpha_minutes*totalminutes) * price
#
# Recall that alpha_int < 0, so this should clarify that when alpha_minutes is
# positive, customers that use their phones more are less price sensitive, i.e.
# they experience less disutility from higher prices than customers who have
# lower total_minutes
# Let's take a closer look at two customers to see how this heterogeneous model
# predicts choice behavior.
# grab the data for 2 customers (#2 & #31) (both with no discount)
x1 <- sub1 |>
filter(customer_id == 2) |>
mutate(
A2 = phone_id == "A2",
S1 = phone_id == "S1",
S2 = phone_id == "S2",
H1 = phone_id == "H1",
H2 = phone_id == "H2",
ptm = price * total_minutes
) |>
select(A2, H1, H2, S1, S2, price, ptm) |>
as.matrix()
x2 <- sub1 |>
filter(customer_id == 31) |>
mutate(
A2 = phone_id == "A2",
S1 = phone_id == "S1",
S2 = phone_id == "S2",
H1 = phone_id == "H1",
H2 = phone_id == "H2",
ptm = price * total_minutes
) |>
select(A2, S1, S2, H1, H2, price, ptm) |>
as.matrix()
# notice how the interaction variable (ptm) is different for the two customers
x1
x2
# This is mainly driven by the variation in total minutes for the two customers.
cust_dat |>
slice(2, 31) |>      # quick way to grab rows by row_index
select(total_minutes)
# but also by the different A1 price faced by the two customers
# use our first het demand model to calculate purchase probabilities
# for the six phones for each of the two customers
beta_hat <- coef(out5)
# let's calculate deterministic utility for each customer and each phone
xb1 <- t(x1 %*% beta_hat)    # t is the transpose function; it prints nicer
xb2 <- t(x2 %*% beta_hat)
# now we'll calculate purchase probability for each customer and each phone using the
# MNL market share formula
example_shares <- rbind(
round(exp(xb1) / rowSums(exp(xb1)), 3),
round(exp(xb2) / rowSums(exp(xb2)), 3)
)
colnames(example_shares) <- c("A1", "A2", "S1", "S2", "H1", "H2")
example_shares
# Notice that the probabilities vary across phones.
# The first customer has a relatively high probability of purchasing the small
# Apple phone (A1), whereas this probability is lower for the second customer.
# Conversely customer 2 has a relatively high probability of purchasing the small
# Huawei phone (H1), given their relatively high price sensitivity.
# Now back to modeling...
# Approach 2: Add heterogeneity via discrete segment/brand interactions
# compare brand-only model before/after adding discrete segment/brand interactions
out1 <- mlogit(choice ~ apple + samsung | 0, data = mdat1)
summary(out1)
brand_hit_rate(mdat1, out1)
product_hit_rate(mdat1, out1)
ll_ratio(mdat1, out1)
out6 <- mlogit(choice ~ apple:segment + samsung:segment | 0, data = mdat1)
summary(out6)
brand_hit_rate(mdat1, out6)
product_hit_rate(mdat1, out6)
ll_ratio(mdat1, out6)
# Let's focus on the comparison of coefficients between models 1 and 6.
# How do the segments vary in their Apple brand preference? Samsung?
#      (Remember, these are relative to Huawei preference)
# The homogeneous brand preference is like a weighted average of the segment-specific
# preferences, where segment sizes are the weights
# plot model predictions of each customer's A1 prob(choice==A1) with A1 discount
myplot(mdat1, out6, "A1")
# Next let's enrich brand+price model with discrete segment interactions
out2 <- mlogit(choice ~ apple + samsung + price | 0, data = mdat1)
summary(out2)
brand_hit_rate(mdat1, out2)
product_hit_rate(mdat1, out2)
ll_ratio(mdat1, out2)
out7 <- mlogit(choice ~ apple:segment + samsung:segment + price:segment | 0, data = mdat1)
summary(out7)
brand_hit_rate(mdat1, out7)
product_hit_rate(mdat1, out7)
ll_ratio(mdat1, out7)
# Let's focus on price sensitivity. Adding segment-specific price variables to the
# model enables us to estimate separate price sensitivities for each segment.
# How do segments differ in their price sensitivities?
# How do segments' price sensitivities compare to brand preferences?
# plot model predictions of each customer's A1 prob(choice==A1) with A1 discount
myplot(mdat1, out7, "A1")
# compare brand-dummy + price + size model before/after adding segment interactions
out3 <- mlogit(choice ~ apple + samsung + price + screen_size | 0, data = mdat1)
summary(out3)
brand_hit_rate(mdat1, out3)
product_hit_rate(mdat1, out3)
ll_ratio(mdat1, out3)
out8 <- mlogit(choice ~ apple:segment + samsung:segment + price:segment + screen_size:segment | 0, data = mdat1)
summary(out8)
brand_hit_rate(mdat1, out8)
product_hit_rate(mdat1, out8)
ll_ratio(mdat1, out8)
# plot model predictions of each customer's A1 prob(choice==A1) with A1 discount
myplot(mdat1, out8, "A1")
# Let's construct a model with both segment-specific and individual-specific heterogeneity
out9 <- mlogit(choice ~ apple:segment + samsung:segment + screen_size:segment +
price:segment + price:total_minutes:segment | 0, data = mdat1)
# Where is the individual specific heterogeneity?
summary(out9)
options(scipen=n)  # let's turn off scientific notation
summary(out9)
brand_hit_rate(mdat1, out9)
product_hit_rate(mdat1, out9)
ll_ratio(mdat1, out9)
# plot each individual's probability of choosing phone A1 on discount
myplot(mdat1, out9, "A1")
# let's incorporate the phone dummies with segment-specific beta parameters
out10 <- mlogit(choice ~ A1:segment + A2:segment + S1:segment +
S2:segment + H1:segment+
price:segment + price:total_minutes:segment | 0, data = mdat1)
summary(out10)
brand_hit_rate(mdat1, out10)
product_hit_rate(mdat1, out10)
ll_ratio(mdat1, out10)
myplot(mdat1, out10, "A1")
# OK now let's enrich the model even further. Let's see if customers with higher minutes
#   have heterogeneous valuations of each phone's quality. We will interact each phone
#   ID with minutes and segment. Let's see what it does to fit statistics & predictive performance.
out11 <- mlogit(choice ~ A1:total_minutes:segment + A2:total_minutes:segment + S1:total_minutes:segment
+ S2:total_minutes:segment + H1:total_minutes:segment
+ price:segment:total_minutes + price:segment
+ price:total_minutes
| 0, data = mdat1)
summary(out11)
brand_hit_rate(mdat1, out11)
product_hit_rate(mdat1, out11)
ll_ratio(mdat1, out11)
myplot(mdat1, out11, "A1")
# Now let's do something intentionally silly. Let's interact everything with customer
#    ID. We do not have any reason to believe customer ID predicts phone choice,
#    but let's see what happens to fit statistics & predictive performance.
out12 <- mlogit(choice ~ A1:total_minutes:segment:customer_id + A2:total_minutes:segment:customer_id + S1:total_minutes:segment:customer_id
+ S2:total_minutes:segment:customer_id + H1:total_minutes:segment:customer_id
+ price:segment:total_minutes:customer_id + price:segment:customer_id
+ price:total_minutes:customer_id  + price:customer_id
| 0, data = mdat1)
summary(out12)
brand_hit_rate(mdat1, out12)
product_hit_rate(mdat1, out12)
ll_ratio(mdat1, out12)
myplot(mdat1, out12, "A1")
# What happened to the fit statistics?
# Which models predict better? We might worry about overfitting reducing predictive power
# Let's choose a model based on 10-fold cross validation
# MSPE == 'mean square prediction error'
cv_mspe <- function(model, data, k=10, seed=4321) {
# control psuedo random numbers
set.seed(seed)
# randomly assign each customer (J rows) to one of k folds
N <- length(unique(data$customer_id))   # number of customers
J <- length(unique(data$phone_id))      # number of products
fold <- rep((1:N) %% k + 1, each=J)    # rep replicates list elements
# %% is a modulus element, so what we're doing here is computing the remainder when
# dividing 1:N by (k+1). So mod(customer_id,10)+1 is always an integer between 1
# and 10. We repeat that 6 times for each customer, to match the mdat1 observations
# preallocate the prediction storage
preds <- vector(length=nrow(data))
# loop over folds
for(i in 1:k) {
# create row indices for training & prediction observations
row_ids_train <- fold != i     # which rows to keep in for training
row_ids_test  <- !row_ids_train  # which rows to hold out for prediction
# fit/train model on training data
out  <- mlogit(formula(model), data=data[row_ids_train,])
# predict choice probabilities for prediction data
yhat <- predict(model, newdata = data[row_ids_test,])
# store yhat values for prediction data
preds[row_ids_test] <- as.vector(t(yhat))
}
# calculate mse
mse <- mean((data$choice - preds)^2)
return(mse)
}
# calculate the MSPE's for each model
mspe <- vector(length=12)
mspe[1] <- cv_mspe(out1, mdat1)
mspe[2] <- cv_mspe(out2, mdat1)
mspe[3] <- cv_mspe(out3, mdat1)
mspe[4] <- cv_mspe(out4, mdat1)
mspe[5] <- cv_mspe(out5, mdat1)
mspe[6] <- cv_mspe(out6, mdat1)
mspe[7] <- cv_mspe(out7, mdat1)
mspe[8] <- cv_mspe(out8, mdat1)
mspe[9] <- cv_mspe(out9, mdat1)
mspe[10] <- cv_mspe(out10, mdat1)
mspe[11] <- cv_mspe(out11, mdat1)
mspe[12] <- cv_mspe(out12, mdat1)
# plot the MSPE's to compare them
tibble(mod_id=1:12, mspe=mspe) |>
ggplot(aes(x=mod_id, y=mspe)) +
geom_point() +
geom_line() +
scale_x_continuous(breaks = 1:12)
ylim(c(0, .16))  # adjustment to show the absolute statistical differences
# that adjustment helps to show why mspe is only 1 factor among several when
#    choosing a model specification
# which model has the lowest cross-validated mean-squared-error?
which.min(mspe)
# What does this tell us about model selection?
# One implication would be that adding noise into the model could dramatically
#   worsen its performance.
### HOMEWORK QUESTIONS
# 1. Which model performed better in the cross-validation exercise, out5 or out6? Why?
# 2. How big of a difference did the segment-specific interactions make in model performance?
# 3. How did out10 compare to out11 in cross-validation performance? What conclusion
#       could you draw from that?
# 4. How did out12 compare to out11 in cross-validation performance? What conclusion
#       could you draw from that?
# 5. Re-calculate the cross-validation exercise, using (a) the SAME models out1-out12
#       you estimated using mdat1, and (b) mdat2 in place of mdat1.
#       Re-create the ggplot to visualize each model's MSPE.
#       How does the pattern compare to the MSPE plot using mdat1?
# 6. Re-calculate the cross-validation exercise, using (a) the SAME models out1-out12
#       you estimated using mdat1, and (b) mdat3 in place of mdat1.
#       Re-create the ggplot to visualize each model's MSPE.
#       How does the pattern compare to the MSPE plot using mdat1?
dat_list <- vector(mode = "list", length = n)
# initialize progress bar
# this loop may be a little slow, so let's have a progress bar show us
# how fast the code is running and how long until the loop finishes
pb <- txtProgressBar(min = 1, max = n, style = 3)
# loop for step 1
for (i in 1:n) {
# get cohort, minutes, brand, and size for customer i
i_cohort <- cust_dat |>
slice(i) |>   # this takes row (i) from cust_dat ; specific form of 'filter'
pull(years_ago)   # this takes the value of years_ago from row (i)
i_brand <- cust_dat |>
slice(i) |>
pull(brand)
i_phone_id <- cust_dat |>
slice(i) |>
pull(phone_id)
i_size <- cust_dat |>
slice(i) |>
pull(screen_size)
i_discount <- cust_dat |>
slice(i) |>
pull(discount)
i_segment <- cust_dat |>
slice(i) |>
pull(segment)
i_minutes <- cust_dat |>
slice(i) |>
pull(total_minutes)
# subset the phone data to the 6 phones for the year when the customer purchased
PD <- phone_dat |> filter(years_ago == i_cohort)
# adjust one of the phone's price for the 10% discount, if applicable
PD <- PD |> mutate(price = price - (phone_id == i_discount) * price * 0.1)
# add customer id to PD
PD <- PD |> mutate(customer_id = i)
# convert the one brand variable into a set of 3 brand dummies (ie, binary variables)
PD <- PD |> mutate(
apple = as.integer(brand == "apple"),
huawei = as.integer(brand == "huawei"),
samsung = as.integer(brand == "samsung"),
A1 = as.integer(phone_id == "A1"),
A2 = as.integer(phone_id == "A2"),
H1 = as.integer(phone_id == "H1"),
H2 = as.integer(phone_id == "H2"),
S1 = as.integer(phone_id == "S1"),
S2 = as.integer(phone_id == "S2")
)
# create a binary variable to indicate the chosen phone
# this is going to be the dependent variable in the MNL model (like "y" in OLS)
PD <- PD |>
mutate(choice = (brand == i_brand) & (screen_size == i_size)) |>
mutate(choice = as.integer(choice))
# add segment and total_minutes
PD <- PD |> mutate(segment = i_segment, total_minutes = i_minutes)
# store this 6-row dataset in the i'th position of that list we initialized before the loop
dat_list[[i]] <- PD |> select(
customer_id, phone_id, choice,
apple, huawei, samsung,
A1, A2, H1, H2, S1, S2,
price, screen_size,
segment, total_minutes
)
# update the progress bar to show progress
setTxtProgressBar(pb, i)
}
# clean up -- delete temporary objects from the loop that we don't need anymore
rm(i, i_cohort, i_brand, i_size, i_discount, i_segment, i_minutes, PD, pb)
# Let's take a look at two (out of the n) 6-row datasets:
dat_list[1]
dat_list[100]
# ++++++++++
# Step 2
# ++++++++++
# Now we will stack the n 6-row customer-specific dataframes into one big
# dataframe (that will have n*6 rows)
# rbind operates on dataframes to concatenate rows
# we use do.call in order to stack dat_list rows into a tibble
mnl_dat <- do.call(rbind, dat_list) |>
as_tibble()
rm(dat_list)
# Let's see how this n*6 row dataframe looks
head(mnl_dat, n = 20)
# We will estimate demand for each year separately, since customer preferences may
# have changed across product generations
# Let's split the big (n*6 row) dataframe into 3 dataframes, one for each year.
sub1 <- mnl_dat |> filter(customer_id %in% which(cust_dat$years_ago == 1))
sub2 <- mnl_dat |> filter(customer_id %in% which(cust_dat$years_ago == 2))
sub3 <- mnl_dat |> filter(customer_id %in% which(cust_dat$years_ago == 3))
# ++++++++++
# Step 3
# ++++++++++
# Here, we will convert the 3 'sub' dataframes into dfidx objects.
# To do that, we have to specify the y variable (choice), whether our datasets
# have 6 times as many rows as the original data (shape="long") or 6 times as
# many columns (shape="wide"), and the id variable that groups the set of
# phones from one choice-occasion together (our "customer_id" variable).
mdat1 <- dfidx::dfidx(sub1, choice = "choice", shape = "long", chid.var = "customer_id")
mdat2 <- dfidx::dfidx(sub2, choice = "choice", shape = "long", chid.var = "customer_id")
mdat3 <- dfidx::dfidx(sub3, choice = "choice", shape = "long", chid.var = "customer_id")
# Let's save these datasets to our hard disks so we don't need to re-run the big
# for-loop above in order to work with these data in the future
# The save() commands lets us save multiple R objects into one file on disk.
# We can then import these saved object in the future using the load() command.
save(sub1, sub2, sub3, mdat1, mdat2, mdat3, file = "2025mgt100-data/mnl_datasets.RData")
n <- nrow(cust_dat)
#load("../data/mnl_datasets.RData")
#load("../data/mnl_performance_functions.RData")# import customer data
# fit het mnl model which performed best in 10-fold cross-validation
out10 <- mlogit(choice ~ A1:segment + A2:segment + S1:segment +
S2:segment + H1:segment+
price:segment +
price:total_minutes:segment | 0, data = mdat1)
# Demand curve for a phone
# Let's estimate the residual demand curve for the small Samsung phone (S1).
# That means we will vary the price of S1 while holding everything else constant,
# and track how S1's market share changes as we change its price.
# Recall that last year S1 had regular price $799 and market share of 25.3%.
# We'll search prices between $599 and $999, in increments of $10
pvec <- seq(from = -200, to = 200, by = 10)
# We'll store market share predictions here in this empty matrix at each price
smat <- matrix(NA, nrow = length(pvec), ncol = 6)
colnames(smat) <- c("A1", "A2", "H1", "H2", "S1", "S2")
# loop over the price change values
for (i in 1:length(pvec)) {
# print progress
cat("Working on", i, "of", length(pvec), "\n")
# get the price change amount
p <- pvec[i]
# change prices but only for S1 phones
tempdat <- as_tibble(mdat1) |>
mutate(price = ifelse(idx$phone_id == "S1", price + p, price))
# make market share predictions with the temporarily-changed S1 prices
preds <- predict(out10, newdata = tempdat)
## Let's take a moment to appreciate how compact that one line of code is.
## What did it accomplish?
## Specialized design features like this help explain why many analytics professionals use R
# calculate and store market shares
smat[i, ] <- colMeans(preds)
}
# What's in smat? Does it make sense?
View(smat)
# Can you see what the optimal price for S1 is going to be?
# gather our prices and estimated shares into a dataframe
relcol <- which(colnames(smat) == "S1")   # "which" picks elements that are true
s1dat <- tibble(scenario = 1:length(pvec), price = pvec + 799, share = smat[, relcol])
# Looking at your raw data is a very good habit to cultivate
View(s1dat)
# Look OK?
# plot S1's inverse residual demand curve
ggplot(s1dat, aes(x = share, y = price)) +
geom_point() +
geom_line() +
labs(x = "Share", y = "Price") +
theme_bw()
# Congratulations! You just estimated your first demand curve!
# Note : we call it "residual" demand when it's product-specific and takes other
# factors as given. That acknowledges that when other factors change, like say
# prices of competing products A1 or H1, then S1 residual demand curve would change also.
# actual market shares
cust_dat |>
filter(years_ago == 1) |>
count(phone_id) |>
mutate(shr = n / sum(n))
# predicted market shares at 0 price change
smat[21, ] |> round(3)
# Convert shares to number of phones
# Suppose the US smartphone market size is 150 million units sold per year
# and further suppose that the college-age demographic that we've measured with our
# dataset comprises 1 out of every 15 smartphone sales, or 10 million phones.
M <- 10    # measured in millions
# Let's scale our demand curve to be in the price-quantity space instead of the price-share space
s1dat <- s1dat |> mutate(quantity = share * M)
ggplot(s1dat, aes(x = quantity, y = price)) +
geom_point() +
geom_line() +
labs(x = "Quantity", y = "Price") +
theme_bw()
# Let's consider pricing S1 based on own-price elasticity to maximize contribution
# Marginal cost
# We need cost data else we cannot measure margin or contribution. Suppose a Samsung manager
# informs us that MC is a constant $470 per S1 phone, invariant to quantity produced.
mc1 <- 470
# Calculate own-price elasticity at +/- $10 from actual price of $799
p1 <- s1dat |>
slice(20) |>     # quick way to filter a row based on row index
pull(price)       # quick way to select a single column
q1 <- s1dat |>
slice(20) |>
pull(quantity)
p2 <- s1dat |>
slice(22) |>
pull(price)
q2 <- s1dat |>
slice(22) |>
pull(quantity)
# this is an approximate price elasticity based on a $20 range around observed price,
# from $789 to $809
elasticity <- ((q2 - q1) / q1) / ((p2 - p1) / p1)
elasticity
# Approximate optimal price using Tucker's elasticity heuristic
mc1 * 1 / (1 - 1 / abs(elasticity))
p1,q1,p2,q2
p1
q1
p2
q2
